{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "65804c76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65804c76",
        "outputId": "bb1ef1c8-01b5-4330-9872-4ebffacaf354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def set_seed(s=2555304):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "\n",
        "def running_in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4ee579af",
      "metadata": {
        "id": "4ee579af"
      },
      "outputs": [],
      "source": [
        "fileList  = [\n",
        "    'X1_train.pkl','X2_train.pkl','X3_train.pkl',\n",
        "    'X1_test.pkl','X2_test.pkl','X3_test.pkl',\n",
        "    'y_train_.pkl','y_test_.pkl'\n",
        "]\n",
        "varNames = [\n",
        "    'X1_train','X2_train','X3_train',\n",
        "    'X1_test','X2_test','X3_test',\n",
        "    'y_train_','y_test_'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1d2716a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d2716a8",
        "outputId": "a4a626b8-b91d-40ff-bd0d-7955fc9fcbbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if running_in_colab():\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  for i, fileItem in enumerate(fileList):\n",
        "    fileList[i] = \"/content/drive/MyDrive/\" + fileItem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "56d9abe0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56d9abe0",
        "outputId": "ee09ef0e-df2a-45d2-cb23-6bdbe78e373c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2027356381.py:4: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  loaded[name] = pickle.load(f)\n"
          ]
        }
      ],
      "source": [
        "loaded = {}\n",
        "for name, fname in zip(varNames, fileList):\n",
        "    with open(fname, 'rb') as f:\n",
        "        loaded[name] = pickle.load(f)\n",
        "\n",
        "globals().update(loaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ef712642",
      "metadata": {
        "id": "ef712642"
      },
      "outputs": [],
      "source": [
        "def loader_function(X, y=None, batch_size=64, shuffle=True, num_workers=4, pin_memory=True, normalize=True):\n",
        "    \"\"\"\n",
        "    X: either a single np.ndarray or a list/tuple of arrays.\n",
        "       Each array can be (N,H,W), (N,C,H,W) or (N,H,W,C).\n",
        "       All arrays must share the same N,H,W. Channels are concatenated.\n",
        "    y: optional labels (N,).\n",
        "    \"\"\"\n",
        "    def to_nchw(a):\n",
        "        a = np.asarray(a)\n",
        "        assert a.ndim in (3,4), \"Each X must be (N,H,W) or (N,C,H,W) or (N,H,W,C)\"\n",
        "        if a.ndim == 3:                 # (N,H,W) -> (N,1,H,W)\n",
        "            a = a[:, None, :, :]\n",
        "        elif a.ndim == 4:\n",
        "            # heuristics: move channels-last to channels-first if needed\n",
        "            if a.shape[1] not in (1,2,3) and a.shape[-1] in (1,2,3):\n",
        "                a = np.transpose(a, (0,3,1,2))\n",
        "        return a.astype(np.float32, copy=False)\n",
        "\n",
        "    # Stack multiple inputs along channel dim\n",
        "    if isinstance(X, (list, tuple)):\n",
        "        xs = [to_nchw(a) for a in X]\n",
        "        N, _, H, W = xs[0].shape\n",
        "        for a in xs[1:]:\n",
        "            assert a.shape[0]==N and a.shape[2]==H and a.shape[3]==W, \"Mismatched N/H/W across inputs\"\n",
        "        Xn = np.ascontiguousarray(np.concatenate(xs, axis=1))   # (N, C_total, H, W)\n",
        "    else:\n",
        "        Xn = np.ascontiguousarray(to_nchw(X))                   # (N, C, H, W)\n",
        "\n",
        "    class _DS(Dataset):\n",
        "        def __init__(self, Xn, y, normalize):\n",
        "            self.X = Xn\n",
        "            self.y = None if y is None else np.asarray(y)\n",
        "            self.normalize = normalize\n",
        "        def __len__(self): return self.X.shape[0]\n",
        "        def __getitem__(self, i):\n",
        "            x = torch.from_numpy(self.X[i])  # (C,H,W), float32\n",
        "            if self.normalize:\n",
        "                m = x.mean(dim=(1,2), keepdim=True)\n",
        "                s = x.std(dim=(1,2), keepdim=True, unbiased=False)\n",
        "                x = (x - m) / (s + 1e-8)\n",
        "                x = torch.nan_to_num(x)\n",
        "            if self.y is None:\n",
        "                return x\n",
        "            yy = self.y[i]\n",
        "            if not torch.is_tensor(yy):\n",
        "                try: yy = torch.tensor(yy, dtype=torch.long)\n",
        "                except Exception: pass\n",
        "            return x, yy\n",
        "\n",
        "    ds = _DS(Xn, y, normalize)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "344ef540",
      "metadata": {
        "id": "344ef540"
      },
      "outputs": [],
      "source": [
        "def train_vqvae(\n",
        "    model, train_loader, val_loader, optimizer,\n",
        "    device=\"cuda\", epochs=100,\n",
        "    beta_start=0.1, beta_end=None, warmup_epochs=2, ramp_epochs=20\n",
        "):\n",
        "    os.makedirs(\"output_images\", exist_ok=True)\n",
        "    model.to(device)\n",
        "\n",
        "    if beta_end is None:\n",
        "        beta_end = float(model.quantizer.beta)\n",
        "\n",
        "    train_total, val_total = [], []\n",
        "    train_recon, train_vq = [], []\n",
        "    code_usage_hist, perplexity_hist = [], []\n",
        "\n",
        "    K = getattr(model.quantizer, \"num_embeddings\", None)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # ---------------- beta schedule + quantize gate ----------------\n",
        "        if epoch <= warmup_epochs:\n",
        "            beta_now = beta_start\n",
        "            quantize_now = False\n",
        "        elif epoch <= warmup_epochs + ramp_epochs:\n",
        "            p = (epoch - warmup_epochs) / float(ramp_epochs)\n",
        "            beta_now = beta_start + p * (beta_end - beta_start)\n",
        "            quantize_now = True\n",
        "        else:\n",
        "            beta_now = beta_end\n",
        "            quantize_now = True\n",
        "        model.quantizer.beta = float(beta_now)\n",
        "\n",
        "        # --------------------------- TRAIN ----------------------------\n",
        "        model.train()\n",
        "        total_loss_sum = 0.0\n",
        "        recon_loss_sum = 0.0\n",
        "        vq_loss_sum = 0.0\n",
        "        n_train = 0\n",
        "\n",
        "        counts_epoch = None if K is None else np.zeros(K, dtype=np.int64)\n",
        "\n",
        "        for batch in train_loader:\n",
        "            # unwrap (x, y) -> x, or list/tuple -> first element\n",
        "            xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            out = model(xb, quantize=quantize_now)  # expects dict with total/recon/vq/indices\n",
        "            loss = out[\"total_loss\"]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            n_train += bs\n",
        "            total_loss_sum += out[\"total_loss\"].item() * bs\n",
        "            recon_loss_sum += out[\"recon_loss\"].item() * bs\n",
        "            vq_loss_sum    += out[\"vq_loss\"].item() * bs\n",
        "\n",
        "            # accumulate code counts (epoch-level) for usage/perplexity\n",
        "            if quantize_now and (K is not None) and (\"indices\" in out) and (out[\"indices\"] is not None):\n",
        "                idx = out[\"indices\"]\n",
        "                if isinstance(idx, (list, tuple)):\n",
        "                    idx = torch.stack(idx)\n",
        "                idx = idx.view(-1).detach().to(\"cpu\")\n",
        "                c = torch.bincount(idx, minlength=K).numpy()\n",
        "                counts_epoch += c\n",
        "\n",
        "        train_total.append(total_loss_sum / n_train)\n",
        "        train_recon.append(recon_loss_sum / n_train)\n",
        "        train_vq.append(vq_loss_sum / n_train)\n",
        "\n",
        "        # ---- epoch code usage & perplexity ----\n",
        "        if (counts_epoch is not None) and (counts_epoch.sum() > 0):\n",
        "            code_usage = int((counts_epoch > 0).sum())\n",
        "            p = counts_epoch / counts_epoch.sum()\n",
        "            nz = p > 0\n",
        "            H = float(-(p[nz] * np.log(p[nz])).sum())\n",
        "            perplexity = float(np.exp(H))\n",
        "        else:\n",
        "            code_usage = 0\n",
        "            perplexity = float(\"nan\")\n",
        "        code_usage_hist.append(code_usage)\n",
        "        perplexity_hist.append(perplexity)\n",
        "\n",
        "        # -------------------------- VALIDATE --------------------------\n",
        "        model.eval()\n",
        "        val_sum = 0.0\n",
        "        n_val = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "\n",
        "                out = model(xb, quantize=quantize_now)\n",
        "                bs = xb.size(0)\n",
        "                n_val += bs\n",
        "                val_sum += out[\"total_loss\"].item() * bs\n",
        "\n",
        "        val_total.append(val_sum / n_val)\n",
        "\n",
        "        print(f\"[Epoch {epoch:03d}] β={beta_now:.3f} \"\n",
        "              f\"Q={'on' if quantize_now else 'off'} | \"\n",
        "              f\"Train {train_total[-1]:.4f} | Val {val_total[-1]:.4f} | \"\n",
        "              f\"Recon {train_recon[-1]:.4f} | VQ {train_vq[-1]:.4f} | \"\n",
        "              f\"Codes {code_usage_hist[-1]} | Perp {perplexity_hist[-1]:.1f}\")\n",
        "\n",
        "    # ---------------------------- PLOTS ------------------------------\n",
        "    ep = range(1, epochs + 1)\n",
        "    plt.figure(figsize=(14, 9))\n",
        "    plt.subplot(2,3,1); plt.plot(ep, train_total, label='Train'); plt.plot(ep, val_total, label='Val'); plt.title(\"Total Loss\"); plt.legend()\n",
        "    plt.subplot(2,3,2); plt.plot(ep, train_recon, label='Recon'); plt.title(\"Reconstruction Loss\"); plt.legend()\n",
        "    plt.subplot(2,3,3); plt.plot(ep, train_vq, label='VQ'); plt.title(\"VQ Loss\"); plt.legend()\n",
        "    plt.subplot(2,3,4); plt.plot(ep, code_usage_hist, label='Unique Codes'); plt.title(\"Code Usage\"); plt.legend()\n",
        "    plt.subplot(2,3,5); plt.plot(ep, perplexity_hist, label='Perplexity'); plt.title(\"Codebook Perplexity\"); plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"output_images/Training_graphs.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # return {\n",
        "    #     \"train_total\": train_total,\n",
        "    #     \"val_total\": val_total,\n",
        "    #     \"train_recon\": train_recon,\n",
        "    #     \"train_vq\": train_vq,\n",
        "    #     \"code_usage\": code_usage_hist,\n",
        "    #     \"perplexity\": perplexity_hist,\n",
        "    # }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6e0f2b3a",
      "metadata": {
        "id": "6e0f2b3a"
      },
      "outputs": [],
      "source": [
        "TrainLoad = loader_function([X1_train, X2_train, X3_train], y_train_, num_workers=0)\n",
        "TestLoad  = loader_function([X1_test,  X2_test,  X3_test],  y_test_,  num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d74b7ac6",
      "metadata": {
        "id": "d74b7ac6"
      },
      "outputs": [],
      "source": [
        "from vqmodelMulti import VQVAE_Multi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "db739385",
      "metadata": {
        "id": "db739385"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = VQVAE_Multi(in_channels=3, embedding_dim=128, num_embeddings=256, beta=0.25).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d12931bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d12931bb",
        "outputId": "a0c8992f-b074-48aa-8450-94046b1a24e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 001] β=0.010 Q=off | Train 0.5734 | Val 0.2259 | Recon 0.5734 | VQ 0.0000 | Codes 0 | Perp nan\n",
            "[Epoch 002] β=0.010 Q=off | Train 0.1503 | Val 0.0975 | Recon 0.1503 | VQ 0.0000 | Codes 0 | Perp nan\n"
          ]
        }
      ],
      "source": [
        "# train_vqvae(model, TrainLoad, TestLoad, optimizer, epochs=100)\n",
        "\n",
        "train_vqvae(model, TrainLoad, TestLoad, optimizer,\n",
        "            epochs=200, beta_start=0.01, beta_end=0.25, warmup_epochs=2, ramp_epochs=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WJpw8LCyxpi8",
      "metadata": {
        "id": "WJpw8LCyxpi8"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"model pt files\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"model pt files/vqvae_model.pth\")\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/vqvae_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b123cb99",
      "metadata": {
        "id": "b123cb99"
      },
      "outputs": [],
      "source": [
        "import numpy as np, torch\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def get_latent_features(model, loader, device=\"cuda\", source=\"zq\", pool=\"meanstd\", ze_merge=\"concat\"):\n",
        "    \"\"\"\n",
        "    source: 'zq' (quantized) or 'ze' (pre-quantization)\n",
        "    pool  : 'meanstd' | 'mean' | 'flat'\n",
        "    ze_merge (multi-encoder only): 'concat' (default) or 'mean' to combine branches.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    X, y = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "            yb = batch[1] if (isinstance(batch, (list, tuple)) and len(batch) > 1) else None\n",
        "            xb = xb.to(device)\n",
        "\n",
        "            if source == \"zq\":\n",
        "                z = model(xb, quantize=True)[\"z_q\"]  # shared z_q: [B,C,H,W]\n",
        "                zs = [z]                             # treat like single branch\n",
        "            else:\n",
        "                # single-encoder vs multi-encoder\n",
        "                if hasattr(model, \"encoders\"):       # multi-encoder model\n",
        "                    xs = [xb[:, i:i+1] for i in range(xb.shape[1])]\n",
        "                    zs = [enc(xi) for enc, xi in zip(model.encoders, xs)]  # list of [B,C,H,W]\n",
        "                else:\n",
        "                    zs = [model.encoder(xb)]\n",
        "\n",
        "            # pool per branch\n",
        "            feats = []\n",
        "            for z in zs:\n",
        "                if pool == \"meanstd\":\n",
        "                    m = z.mean((2,3)); s = z.std((2,3))\n",
        "                    f = torch.cat([m, s], dim=1)\n",
        "                elif pool == \"mean\":\n",
        "                    f = z.mean((2,3))\n",
        "                else:  # 'flat'\n",
        "                    f = z.permute(0,2,3,1).reshape(z.size(0), -1)\n",
        "                feats.append(f)\n",
        "\n",
        "            # merge branches\n",
        "            if len(feats) == 1:\n",
        "                feat = feats[0]\n",
        "            else:\n",
        "                if ze_merge == \"mean\":\n",
        "                    feat = torch.stack(feats, dim=0).mean(dim=0)   # [B,d]\n",
        "                else:\n",
        "                    feat = torch.cat(feats, dim=1)                 # [B, d*branches]\n",
        "\n",
        "            X.append(feat.cpu().numpy())\n",
        "            if yb is not None:\n",
        "                y.append(yb.detach().cpu().numpy() if torch.is_tensor(yb) else np.asarray(yb))\n",
        "\n",
        "    X = np.concatenate(X, axis=0)\n",
        "    y = None if not y else np.concatenate(y)\n",
        "    return X, y\n",
        "\n",
        "def plot_latent_2d(X, y=None, title=\"Latent space (t-SNE)\"):\n",
        "    Xp = PCA(n_components=min(50, X.shape[1]-1)).fit_transform(X)\n",
        "    X2 = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30).fit_transform(Xp)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    if y is None:\n",
        "        plt.scatter(X2[:,0], X2[:,1], s=6)\n",
        "    else:\n",
        "        y_arr = np.asarray(y)\n",
        "        y_plot = LabelEncoder().fit_transform(y_arr.astype(str)) if y_arr.dtype.kind not in \"iu\" else y_arr\n",
        "        plt.scatter(X2[:,0], X2[:,1], c=y_plot, s=6, cmap=\"tab10\")\n",
        "    plt.title(title); plt.tight_layout(); plt.show()\n",
        "    return X2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f5faa7",
      "metadata": {
        "id": "55f5faa7"
      },
      "outputs": [],
      "source": [
        "# Quantized latent (unchanged)\n",
        "Xzq, y = get_latent_features(model, TestLoad, device=device, source=\"zq\", pool=\"meanstd\")\n",
        "_ = plot_latent_2d(Xzq, y, \"z_q (mean+std) t-SNE\")\n",
        "\n",
        "# Pre-quantization (multi-encoder aware)\n",
        "Xze, y = get_latent_features(model, TestLoad, device=device, source=\"ze\", pool=\"meanstd\", ze_merge=\"concat\")\n",
        "_ = plot_latent_2d(Xze, y, \"z_e (mean+std, concat) t-SNE\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b59a2d",
      "metadata": {
        "id": "a9b59a2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np, torch, matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def code_hist_features(model, loader, device=\"cuda\", norm=\"hellinger\"):\n",
        "    \"\"\"\n",
        "    Build per-sample histograms over VQ code indices.\n",
        "    norm: 'none' | 'l1' | 'hellinger' (L1 -> sqrt -> L2)\n",
        "    Returns: X [N,K], y (or None)\n",
        "    \"\"\"\n",
        "    K = model.quantizer.num_embeddings\n",
        "    model.eval(); X, y = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "            yb = batch[1] if (isinstance(batch, (list, tuple)) and len(batch) > 1) else None\n",
        "            idx = model(xb.to(device), quantize=True)[\"indices\"].view(xb.size(0), -1)\n",
        "            for i in range(idx.size(0)):\n",
        "                h = torch.bincount(idx[i], minlength=K).float()\n",
        "                if norm in (\"l1\", \"hellinger\"):\n",
        "                    h = h / (h.sum() + 1e-8)\n",
        "                if norm == \"hellinger\":\n",
        "                    h = torch.sqrt(h); h = h / (h.norm() + 1e-8)\n",
        "                X.append(h.cpu().numpy())\n",
        "            if yb is not None:\n",
        "                y.append(yb.detach().cpu().numpy() if torch.is_tensor(yb) else np.asarray(yb))\n",
        "    X = np.stack(X).astype(np.float32)\n",
        "    y = None if not y else np.concatenate(y)\n",
        "    return X, y\n",
        "\n",
        "def plot_hist_tsne(X, y=None, title=\"t-SNE of code histograms\"):\n",
        "    Xp = PCA(n_components=min(50, X.shape[1]-1)).fit_transform(X)\n",
        "    X2 = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30).fit_transform(Xp)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    if y is None:\n",
        "        plt.scatter(X2[:,0], X2[:,1], s=6)\n",
        "    else:\n",
        "        y_arr = np.asarray(y)\n",
        "        y_plot = LabelEncoder().fit_transform(y_arr.astype(str)) if y_arr.dtype.kind not in \"iu\" else y_arr\n",
        "        plt.scatter(X2[:,0], X2[:,1], c=y_plot, s=6, cmap=\"tab10\")\n",
        "    plt.title(title); plt.tight_layout(); plt.show()\n",
        "    return X2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab701b52",
      "metadata": {
        "id": "ab701b52"
      },
      "outputs": [],
      "source": [
        "Xh, y = code_hist_features(model, TrainLoad, device=device, norm=\"hellinger\")\n",
        "_ = plot_hist_tsne(Xh, y, title=\"BoW codes (Hellinger) t-SNE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b56b64",
      "metadata": {
        "id": "b7b56b64"
      },
      "outputs": [],
      "source": [
        "def plot_mean_code_usage(X_l1):\n",
        "    mean_h = X_l1.mean(axis=0)\n",
        "    plt.figure(figsize=(8,3))\n",
        "    plt.bar(np.arange(len(mean_h)), mean_h)\n",
        "    plt.title(\"Dataset-average code usage\"); plt.xlabel(\"Code index\"); plt.ylabel(\"Mean weight\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# call with L1 features\n",
        "Xl1, _ = code_hist_features(model, TrainLoad, device=device, norm=\"l1\")\n",
        "plot_mean_code_usage(Xl1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d73c3839",
      "metadata": {
        "id": "d73c3839"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5331786",
      "metadata": {
        "id": "f5331786"
      },
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load(\"vqvae_model.pth\"))\n",
        "\n",
        "FEAT = \"bow\"\n",
        "\n",
        "if FEAT == \"bow\":\n",
        "    Xtr, ytr = code_hist_features(model, TrainLoad, device=device, norm=\"hellinger\")\n",
        "    Xte, yte = code_hist_features(model, TestLoad,  device=device, norm=\"hellinger\")\n",
        "    metric = \"cosine\"     # best for histograms\n",
        "    use_pca = 64          # optional small PCA\n",
        "else:  # \"latent\"\n",
        "    Xtr, ytr = get_latent_features(model, TrainLoad, device=device, source=\"zq\", pool=\"meanstd\")\n",
        "    Xte, yte = get_latent_features(model, TestLoad,  device=device, source=\"zq\", pool=\"meanstd\")\n",
        "    metric = \"euclidean\"\n",
        "    use_pca = 64          # optional; set None to skip\n",
        "\n",
        "# --- 2) tiny KNN CV + test eval ---\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "ks = [3,5,7,11,15,21,31]\n",
        "\n",
        "def make_pipe(metric, pca_dim):\n",
        "    ests = []\n",
        "    ests.append(Normalizer(norm=\"l2\") if metric == \"cosine\" else StandardScaler())\n",
        "    if pca_dim:\n",
        "        ests.append(PCA(n_components=pca_dim, whiten=True, random_state=42))\n",
        "    ests.append(KNeighborsClassifier(metric=metric, weights=\"distance\"))\n",
        "    return make_pipeline(*ests)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_means, cv_stds = [], []\n",
        "\n",
        "for k in ks:\n",
        "    pipe = make_pipe(metric, use_pca)\n",
        "    pipe.set_params(kneighborsclassifier__n_neighbors=k)\n",
        "    scores = cross_val_score(pipe, Xtr, ytr, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
        "    cv_means.append(scores.mean()); cv_stds.append(scores.std())\n",
        "\n",
        "best_k = ks[int(np.argmax(cv_means))]\n",
        "best_pipe = make_pipe(metric, use_pca).set_params(kneighborsclassifier__n_neighbors=best_k)\n",
        "best_pipe.fit(Xtr, ytr)\n",
        "test_acc = best_pipe.score(Xte, yte)\n",
        "\n",
        "print(f\"Best k={best_k} | CV acc={max(cv_means):.3f} (+/- {cv_stds[np.argmax(cv_means)]:.3f}) \"\n",
        "      f\"| Test acc={test_acc:.3f}\")\n",
        "\n",
        "# (optional) plot CV curve\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.errorbar(ks, cv_means, yerr=cv_stds, marker=\"o\")\n",
        "plt.xlabel(\"k\"); plt.ylabel(\"CV accuracy\"); plt.title(f\"KNN ({FEAT}, metric={metric})\")\n",
        "plt.grid(True, alpha=0.3); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2f3bdc",
      "metadata": {
        "id": "cd2f3bdc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "text_analytics",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
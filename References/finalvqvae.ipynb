{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "58cfdc47",
      "metadata": {
        "id": "58cfdc47"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "# import faiss\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7a1bc8",
      "metadata": {
        "id": "bb7a1bc8"
      },
      "source": [
        "# VQVAE Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "TYEDIoC2RB6j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYEDIoC2RB6j",
        "outputId": "8fa9a3f8-7bcf-4bc8-f9d3-2121f58d2dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7d062f5b",
      "metadata": {
        "id": "7d062f5b"
      },
      "outputs": [],
      "source": [
        "# with open(\"pkl files/chunks.pkl\", \"rb\") as f:\n",
        "#     chunks = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/chunks.pkl\", \"rb\") as f:\n",
        "    chunks = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8d544021",
      "metadata": {
        "id": "8d544021"
      },
      "outputs": [],
      "source": [
        "# Importing models\n",
        "from vqmodel import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "af4d3cf9",
      "metadata": {
        "id": "af4d3cf9"
      },
      "outputs": [],
      "source": [
        "# Dataframe to tensor transition\n",
        "images = []\n",
        "\n",
        "for df in chunks:\n",
        "    # df['ch'] is a Series of 80 columns; each item is a list of 200 values\n",
        "    # Create a (200, 80) NumPy array (transpose is needed)\n",
        "    matrix = np.stack(df['PWR_ch1'].to_list(), axis=1)  # shape: (200, 80)\n",
        "    images.append(matrix)\n",
        "\n",
        "# Convert the whole thing to numpy because making tensors from a list of arrays\n",
        "# is extremely slow\n",
        "images_array = np.array(images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f8e389b0",
      "metadata": {
        "id": "f8e389b0"
      },
      "outputs": [],
      "source": [
        "# Convert to torch tensor and add batch + channel dimensions\n",
        "data_tensor = torch.tensor(images_array, dtype=torch.float32)  # (B, 200, 80)\n",
        "data_tensor = data_tensor.unsqueeze(1)  # (B, 1, 200, 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3f64eb1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f64eb1b",
        "outputId": "c0ef2e6b-e739-45e8-8b52-7333f0b3a14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1, 200, 30])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = ChunkImageDataset(chunks)\n",
        "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "model = VQVAE(in_channels=1)\n",
        "\n",
        "for batch in loader:\n",
        "    print(batch.shape)  # (8, 1, 200, 80)\n",
        "    outputs = model(batch)\n",
        "    break  # for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "96193a4d",
      "metadata": {
        "id": "96193a4d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. First split: 80% train, 20% temp\n",
        "train_chunks, temp_chunks = train_test_split(\n",
        "    chunks, test_size=0.2, random_state=2555304\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "498f88bd",
      "metadata": {
        "id": "498f88bd"
      },
      "outputs": [],
      "source": [
        "# 2. Split temp into 10% val, 10% test\n",
        "val_chunks, test_chunks = train_test_split(\n",
        "    temp_chunks, test_size=0.5, random_state=2555304\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "00fa57df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00fa57df",
        "outputId": "47a0d1d0-b239-41d7-ff74-42ef51b59e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 5147\n",
            "Validation: 643\n",
            "Test: 644\n"
          ]
        }
      ],
      "source": [
        "# Check counts\n",
        "print(f\"Train: {len(train_chunks)}\")\n",
        "print(f\"Validation: {len(val_chunks)}\")\n",
        "print(f\"Test: {len(test_chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8e4dacf8",
      "metadata": {
        "id": "8e4dacf8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Wrap into datasets\n",
        "train_dataset = ChunkImageDataset(train_chunks)\n",
        "val_dataset = ChunkImageDataset(val_chunks)\n",
        "test_dataset = ChunkImageDataset(test_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5d092207",
      "metadata": {
        "id": "5d092207"
      },
      "outputs": [],
      "source": [
        "# Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8be8bc8",
      "metadata": {
        "id": "e8be8bc8"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e61cba20",
      "metadata": {
        "id": "e61cba20"
      },
      "outputs": [],
      "source": [
        "def train_vqvae(model, train_loader, val_loader, optimizer, device=\"cuda\", epochs=500):\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize plot variables\n",
        "    train_losses, val_losses = [], []\n",
        "    recon_losses, vq_losses = [], []\n",
        "    cb_losses, cm_losses = [], []\n",
        "    code_usages, zq_stds = [], []\n",
        "    perplexities = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, recon_total, cb_total, cm_total, vq_total = 0, 0, 0, 0, 0\n",
        "        code_indices_set = set()\n",
        "        zq_std_list = []\n",
        "        epoch_indices = []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            out[\"total_loss\"].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += out[\"total_loss\"].item() * batch.size(0)\n",
        "            recon_total += out[\"recon_loss\"].item() * batch.size(0)\n",
        "            cb_total += out[\"codebook_loss\"].item() * batch.size(0)\n",
        "            cm_total += out[\"commitment_loss\"].item() * batch.size(0)\n",
        "            vq_total += out[\"vq_loss\"].item() * batch.size(0)\n",
        "            code_indices_set.update(out[\"indices\"].detach().cpu().numpy().tolist())\n",
        "\n",
        "            epoch_indices.append(out[\"indices\"].detach().cpu().numpy())\n",
        "            zq_std_list.append(out[\"z_q\"].std().item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch)\n",
        "                val_loss += F.mse_loss(out[\"recon_x\"], batch).item() * batch.size(0)\n",
        "\n",
        "        train_losses.append(total_loss / len(train_loader.dataset))\n",
        "        val_losses.append(val_loss / len(val_loader.dataset))\n",
        "        recon_losses.append(recon_total / len(train_loader.dataset))\n",
        "        cb_losses.append(cb_total / len(train_loader.dataset))\n",
        "        cm_losses.append(cm_total / len(train_loader.dataset))\n",
        "        vq_losses.append(vq_total / len(train_loader.dataset))\n",
        "        code_usages.append(len(code_indices_set))\n",
        "        zq_stds.append(np.mean(zq_std_list))\n",
        "\n",
        "        # At the end of each epoch:\n",
        "        all_indices = np.concatenate(epoch_indices)  # where epoch_indices is a list of all batch indices\n",
        "        perplexity = compute_perplexity(all_indices, model.quantizer.num_embeddings)\n",
        "        perplexities.append(perplexity)\n",
        "\n",
        "        print(f\"[Epoch {epoch}] Train Loss: {train_losses[-1]:.4f} | \"\n",
        "              f\"Val Loss: {val_losses[-1]:.4f} | \"\n",
        "              f\"Codes Used: {code_usages[-1]} | \"\n",
        "              f\"Perplexity: {perplexity:.2f}\")\n",
        "        print(f\"z_q mean: {out['z_q'].mean().item():.6f} | std: {zq_stds[-1]:.6f}\")\n",
        "\n",
        "    # Plotting\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(2, 4, 1)\n",
        "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs_range, val_losses, label='Val Loss')\n",
        "    plt.title(\"Total Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 4, 2)\n",
        "    plt.plot(epochs_range, recon_losses, label='Reconstruction Loss')\n",
        "    plt.title(\"Reconstruction Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 4, 3)\n",
        "    plt.plot(epochs_range, cb_losses, label='Codebook Loss')\n",
        "    plt.title(\"Codebook Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 4, 4)\n",
        "    plt.plot(epochs_range, cm_losses, label='Commitment Loss')\n",
        "    plt.title(\"Commitment Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 4, 5)\n",
        "    plt.plot(epochs_range, vq_losses, label='VQ Loss')\n",
        "    plt.title(\"VQ Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 4, 6)\n",
        "    plt.plot(epochs_range, code_usages, label='Unique Codes Used')\n",
        "    plt.title(\"Codebook Usage\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 4, 7)\n",
        "    plt.plot(perplexities, label=\"Codebook Perplexity\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Perplexity\")\n",
        "    plt.title(\"Codebook Perplexity Over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"output_images/Training_graphs.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d31457b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d31457b",
        "outputId": "b4bbc012-69cb-41a8-e92b-d8e188752ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train Loss: 1.7563 | Val Loss: 0.5050 | Codes Used: 147 | Perplexity: 1.54\n",
            "z_q mean: -0.030737 | std: 0.514232\n",
            "[Epoch 2] Train Loss: 1.9681 | Val Loss: 0.4305 | Codes Used: 54 | Perplexity: 1.86\n",
            "z_q mean: -0.029465 | std: 0.534697\n",
            "[Epoch 3] Train Loss: 1.9636 | Val Loss: 0.3935 | Codes Used: 83 | Perplexity: 2.28\n",
            "z_q mean: -0.027745 | std: 0.558534\n",
            "[Epoch 4] Train Loss: 1.8239 | Val Loss: 0.3585 | Codes Used: 104 | Perplexity: 2.94\n",
            "z_q mean: -0.025629 | std: 0.573061\n",
            "[Epoch 5] Train Loss: 1.7302 | Val Loss: 0.3408 | Codes Used: 127 | Perplexity: 3.76\n",
            "z_q mean: -0.022768 | std: 0.577935\n",
            "[Epoch 6] Train Loss: 1.6926 | Val Loss: 0.3218 | Codes Used: 133 | Perplexity: 4.60\n",
            "z_q mean: -0.019011 | std: 0.580548\n",
            "[Epoch 7] Train Loss: 1.6875 | Val Loss: 0.3092 | Codes Used: 137 | Perplexity: 5.35\n",
            "z_q mean: -0.016928 | std: 0.585990\n",
            "[Epoch 8] Train Loss: 1.6713 | Val Loss: 0.2995 | Codes Used: 119 | Perplexity: 5.91\n",
            "z_q mean: -0.015314 | std: 0.595088\n",
            "[Epoch 9] Train Loss: 1.6223 | Val Loss: 0.2911 | Codes Used: 109 | Perplexity: 6.35\n",
            "z_q mean: -0.015286 | std: 0.606243\n",
            "[Epoch 10] Train Loss: 1.5833 | Val Loss: 0.2848 | Codes Used: 108 | Perplexity: 6.75\n",
            "z_q mean: -0.013660 | std: 0.617233\n",
            "[Epoch 11] Train Loss: 1.5348 | Val Loss: 0.2788 | Codes Used: 106 | Perplexity: 7.16\n",
            "z_q mean: -0.011369 | std: 0.627791\n",
            "[Epoch 12] Train Loss: 1.4818 | Val Loss: 0.2728 | Codes Used: 105 | Perplexity: 7.60\n",
            "z_q mean: -0.010676 | std: 0.636711\n",
            "[Epoch 13] Train Loss: 1.4292 | Val Loss: 0.2696 | Codes Used: 119 | Perplexity: 8.05\n",
            "z_q mean: -0.010102 | std: 0.644543\n",
            "[Epoch 14] Train Loss: 1.3850 | Val Loss: 0.2647 | Codes Used: 119 | Perplexity: 8.53\n",
            "z_q mean: -0.009583 | std: 0.652189\n",
            "[Epoch 15] Train Loss: 1.3413 | Val Loss: 0.2606 | Codes Used: 126 | Perplexity: 9.00\n",
            "z_q mean: -0.009294 | std: 0.659004\n",
            "[Epoch 16] Train Loss: 1.3064 | Val Loss: 0.2567 | Codes Used: 124 | Perplexity: 9.48\n",
            "z_q mean: -0.008686 | std: 0.665459\n",
            "[Epoch 17] Train Loss: 1.2643 | Val Loss: 0.2542 | Codes Used: 127 | Perplexity: 9.91\n",
            "z_q mean: -0.007645 | std: 0.671621\n",
            "[Epoch 18] Train Loss: 1.2337 | Val Loss: 0.2511 | Codes Used: 124 | Perplexity: 10.38\n",
            "z_q mean: -0.007396 | std: 0.676692\n",
            "[Epoch 19] Train Loss: 1.2026 | Val Loss: 0.2478 | Codes Used: 114 | Perplexity: 10.77\n",
            "z_q mean: -0.006442 | std: 0.682217\n",
            "[Epoch 20] Train Loss: 1.1711 | Val Loss: 0.2452 | Codes Used: 108 | Perplexity: 11.14\n",
            "z_q mean: -0.005732 | std: 0.686660\n",
            "[Epoch 21] Train Loss: 1.1405 | Val Loss: 0.2457 | Codes Used: 104 | Perplexity: 11.51\n",
            "z_q mean: -0.004748 | std: 0.691337\n",
            "[Epoch 22] Train Loss: 1.1174 | Val Loss: 0.2417 | Codes Used: 105 | Perplexity: 11.84\n",
            "z_q mean: -0.003894 | std: 0.695896\n",
            "[Epoch 23] Train Loss: 1.0919 | Val Loss: 0.2395 | Codes Used: 103 | Perplexity: 12.18\n",
            "z_q mean: -0.003021 | std: 0.700868\n",
            "[Epoch 24] Train Loss: 1.0717 | Val Loss: 0.2380 | Codes Used: 92 | Perplexity: 12.48\n",
            "z_q mean: -0.001991 | std: 0.705849\n",
            "[Epoch 25] Train Loss: 1.0533 | Val Loss: 0.2368 | Codes Used: 97 | Perplexity: 12.78\n",
            "z_q mean: -0.001274 | std: 0.710988\n",
            "[Epoch 26] Train Loss: 1.0336 | Val Loss: 0.2361 | Codes Used: 94 | Perplexity: 13.11\n",
            "z_q mean: -0.000483 | std: 0.715803\n",
            "[Epoch 27] Train Loss: 1.0134 | Val Loss: 0.2341 | Codes Used: 99 | Perplexity: 13.51\n",
            "z_q mean: 0.000078 | std: 0.719588\n",
            "[Epoch 28] Train Loss: 0.9930 | Val Loss: 0.2316 | Codes Used: 100 | Perplexity: 13.96\n",
            "z_q mean: 0.000944 | std: 0.723067\n",
            "[Epoch 29] Train Loss: 0.9718 | Val Loss: 0.2306 | Codes Used: 104 | Perplexity: 14.38\n",
            "z_q mean: 0.000677 | std: 0.725208\n",
            "[Epoch 30] Train Loss: 0.9553 | Val Loss: 0.2287 | Codes Used: 118 | Perplexity: 14.74\n",
            "z_q mean: 0.001024 | std: 0.726794\n",
            "[Epoch 31] Train Loss: 0.9348 | Val Loss: 0.2277 | Codes Used: 117 | Perplexity: 15.03\n",
            "z_q mean: 0.001511 | std: 0.729506\n",
            "[Epoch 32] Train Loss: 0.9233 | Val Loss: 0.2272 | Codes Used: 109 | Perplexity: 15.27\n",
            "z_q mean: 0.002785 | std: 0.733064\n",
            "[Epoch 33] Train Loss: 0.9062 | Val Loss: 0.2259 | Codes Used: 121 | Perplexity: 15.44\n",
            "z_q mean: 0.003624 | std: 0.737453\n",
            "[Epoch 34] Train Loss: 0.8915 | Val Loss: 0.2251 | Codes Used: 130 | Perplexity: 15.57\n",
            "z_q mean: 0.004043 | std: 0.741991\n",
            "[Epoch 35] Train Loss: 0.8838 | Val Loss: 0.2241 | Codes Used: 126 | Perplexity: 15.73\n",
            "z_q mean: 0.005036 | std: 0.747017\n",
            "[Epoch 36] Train Loss: 0.8706 | Val Loss: 0.2238 | Codes Used: 118 | Perplexity: 15.86\n",
            "z_q mean: 0.006269 | std: 0.751641\n",
            "[Epoch 37] Train Loss: 0.8596 | Val Loss: 0.2223 | Codes Used: 120 | Perplexity: 15.98\n",
            "z_q mean: 0.007123 | std: 0.756313\n",
            "[Epoch 38] Train Loss: 0.8507 | Val Loss: 0.2211 | Codes Used: 133 | Perplexity: 16.13\n",
            "z_q mean: 0.008234 | std: 0.760593\n",
            "[Epoch 39] Train Loss: 0.8441 | Val Loss: 0.2210 | Codes Used: 128 | Perplexity: 16.29\n",
            "z_q mean: 0.008488 | std: 0.765611\n",
            "[Epoch 40] Train Loss: 0.8338 | Val Loss: 0.2197 | Codes Used: 122 | Perplexity: 16.46\n",
            "z_q mean: 0.009526 | std: 0.769579\n",
            "[Epoch 41] Train Loss: 0.8253 | Val Loss: 0.2187 | Codes Used: 127 | Perplexity: 16.66\n",
            "z_q mean: 0.010021 | std: 0.773887\n",
            "[Epoch 42] Train Loss: 0.8182 | Val Loss: 0.2183 | Codes Used: 125 | Perplexity: 16.90\n",
            "z_q mean: 0.011496 | std: 0.777322\n",
            "[Epoch 43] Train Loss: 0.8082 | Val Loss: 0.2174 | Codes Used: 130 | Perplexity: 17.20\n",
            "z_q mean: 0.011760 | std: 0.780117\n",
            "[Epoch 44] Train Loss: 0.8011 | Val Loss: 0.2168 | Codes Used: 127 | Perplexity: 17.53\n",
            "z_q mean: 0.013000 | std: 0.781943\n",
            "[Epoch 45] Train Loss: 0.7907 | Val Loss: 0.2161 | Codes Used: 132 | Perplexity: 17.88\n",
            "z_q mean: 0.012854 | std: 0.783723\n",
            "[Epoch 46] Train Loss: 0.7875 | Val Loss: 0.2148 | Codes Used: 126 | Perplexity: 18.22\n",
            "z_q mean: 0.013041 | std: 0.784396\n",
            "[Epoch 47] Train Loss: 0.7793 | Val Loss: 0.2140 | Codes Used: 128 | Perplexity: 18.57\n",
            "z_q mean: 0.013356 | std: 0.785087\n",
            "[Epoch 48] Train Loss: 0.7735 | Val Loss: 0.2123 | Codes Used: 134 | Perplexity: 18.94\n",
            "z_q mean: 0.014891 | std: 0.785897\n",
            "[Epoch 49] Train Loss: 0.7657 | Val Loss: 0.2121 | Codes Used: 131 | Perplexity: 19.25\n",
            "z_q mean: 0.014942 | std: 0.786401\n",
            "[Epoch 50] Train Loss: 0.7581 | Val Loss: 0.2123 | Codes Used: 132 | Perplexity: 19.58\n",
            "z_q mean: 0.014733 | std: 0.786781\n",
            "[Epoch 51] Train Loss: 0.7533 | Val Loss: 0.2100 | Codes Used: 129 | Perplexity: 19.93\n",
            "z_q mean: 0.013990 | std: 0.787602\n",
            "[Epoch 52] Train Loss: 0.7454 | Val Loss: 0.2093 | Codes Used: 122 | Perplexity: 20.26\n",
            "z_q mean: 0.014242 | std: 0.786758\n",
            "[Epoch 53] Train Loss: 0.7393 | Val Loss: 0.2081 | Codes Used: 125 | Perplexity: 20.59\n",
            "z_q mean: 0.012472 | std: 0.785706\n",
            "[Epoch 54] Train Loss: 0.7321 | Val Loss: 0.2077 | Codes Used: 130 | Perplexity: 20.92\n",
            "z_q mean: 0.012634 | std: 0.784916\n",
            "[Epoch 55] Train Loss: 0.7286 | Val Loss: 0.2075 | Codes Used: 126 | Perplexity: 21.15\n",
            "z_q mean: 0.011346 | std: 0.784558\n",
            "[Epoch 56] Train Loss: 0.7235 | Val Loss: 0.2064 | Codes Used: 130 | Perplexity: 21.40\n",
            "z_q mean: 0.012114 | std: 0.784089\n",
            "[Epoch 57] Train Loss: 0.7221 | Val Loss: 0.2066 | Codes Used: 128 | Perplexity: 21.59\n",
            "z_q mean: 0.012222 | std: 0.784112\n",
            "[Epoch 58] Train Loss: 0.7169 | Val Loss: 0.2055 | Codes Used: 130 | Perplexity: 21.78\n",
            "z_q mean: 0.011090 | std: 0.784941\n",
            "[Epoch 59] Train Loss: 0.7138 | Val Loss: 0.2052 | Codes Used: 126 | Perplexity: 22.00\n",
            "z_q mean: 0.011531 | std: 0.785904\n",
            "[Epoch 60] Train Loss: 0.7094 | Val Loss: 0.2037 | Codes Used: 120 | Perplexity: 22.25\n",
            "z_q mean: 0.011588 | std: 0.787353\n",
            "[Epoch 61] Train Loss: 0.7071 | Val Loss: 0.2031 | Codes Used: 123 | Perplexity: 22.50\n",
            "z_q mean: 0.010915 | std: 0.788880\n",
            "[Epoch 62] Train Loss: 0.7057 | Val Loss: 0.2022 | Codes Used: 129 | Perplexity: 22.75\n",
            "z_q mean: 0.011063 | std: 0.790833\n",
            "[Epoch 63] Train Loss: 0.7037 | Val Loss: 0.2021 | Codes Used: 119 | Perplexity: 23.03\n",
            "z_q mean: 0.010927 | std: 0.792378\n",
            "[Epoch 64] Train Loss: 0.7040 | Val Loss: 0.2009 | Codes Used: 124 | Perplexity: 23.27\n",
            "z_q mean: 0.012404 | std: 0.794776\n",
            "[Epoch 65] Train Loss: 0.7029 | Val Loss: 0.2006 | Codes Used: 121 | Perplexity: 23.55\n",
            "z_q mean: 0.012159 | std: 0.796910\n",
            "[Epoch 66] Train Loss: 0.7030 | Val Loss: 0.2001 | Codes Used: 121 | Perplexity: 23.78\n",
            "z_q mean: 0.012767 | std: 0.799197\n",
            "[Epoch 67] Train Loss: 0.7024 | Val Loss: 0.1993 | Codes Used: 118 | Perplexity: 23.98\n",
            "z_q mean: 0.012879 | std: 0.801732\n",
            "[Epoch 68] Train Loss: 0.6995 | Val Loss: 0.1986 | Codes Used: 125 | Perplexity: 24.22\n",
            "z_q mean: 0.011972 | std: 0.803624\n",
            "[Epoch 69] Train Loss: 0.7001 | Val Loss: 0.1975 | Codes Used: 121 | Perplexity: 24.44\n",
            "z_q mean: 0.012960 | std: 0.806617\n",
            "[Epoch 70] Train Loss: 0.6970 | Val Loss: 0.1986 | Codes Used: 124 | Perplexity: 24.65\n",
            "z_q mean: 0.011161 | std: 0.809497\n",
            "[Epoch 71] Train Loss: 0.6980 | Val Loss: 0.1971 | Codes Used: 125 | Perplexity: 24.93\n",
            "z_q mean: 0.014618 | std: 0.812492\n",
            "[Epoch 72] Train Loss: 0.6950 | Val Loss: 0.1957 | Codes Used: 129 | Perplexity: 25.14\n",
            "z_q mean: 0.012933 | std: 0.815135\n",
            "[Epoch 73] Train Loss: 0.6945 | Val Loss: 0.1953 | Codes Used: 124 | Perplexity: 25.38\n",
            "z_q mean: 0.013949 | std: 0.817865\n",
            "[Epoch 74] Train Loss: 0.6935 | Val Loss: 0.1949 | Codes Used: 130 | Perplexity: 25.58\n",
            "z_q mean: 0.014945 | std: 0.820464\n",
            "[Epoch 75] Train Loss: 0.6915 | Val Loss: 0.1938 | Codes Used: 129 | Perplexity: 25.75\n",
            "z_q mean: 0.014609 | std: 0.823184\n",
            "[Epoch 76] Train Loss: 0.6909 | Val Loss: 0.1939 | Codes Used: 131 | Perplexity: 25.97\n",
            "z_q mean: 0.015039 | std: 0.826270\n",
            "[Epoch 77] Train Loss: 0.6891 | Val Loss: 0.1934 | Codes Used: 129 | Perplexity: 26.13\n",
            "z_q mean: 0.014902 | std: 0.828817\n",
            "[Epoch 78] Train Loss: 0.6879 | Val Loss: 0.1927 | Codes Used: 128 | Perplexity: 26.31\n",
            "z_q mean: 0.015038 | std: 0.831343\n",
            "[Epoch 79] Train Loss: 0.6864 | Val Loss: 0.1933 | Codes Used: 134 | Perplexity: 26.45\n",
            "z_q mean: 0.015052 | std: 0.833875\n",
            "[Epoch 80] Train Loss: 0.6835 | Val Loss: 0.1919 | Codes Used: 135 | Perplexity: 26.62\n",
            "z_q mean: 0.016505 | std: 0.836473\n",
            "[Epoch 81] Train Loss: 0.6843 | Val Loss: 0.1910 | Codes Used: 129 | Perplexity: 26.88\n",
            "z_q mean: 0.015297 | std: 0.839388\n",
            "[Epoch 82] Train Loss: 0.6832 | Val Loss: 0.1920 | Codes Used: 135 | Perplexity: 27.16\n",
            "z_q mean: 0.016878 | std: 0.842162\n",
            "[Epoch 83] Train Loss: 0.6810 | Val Loss: 0.1911 | Codes Used: 128 | Perplexity: 27.44\n",
            "z_q mean: 0.016847 | std: 0.844036\n",
            "[Epoch 84] Train Loss: 0.6794 | Val Loss: 0.1905 | Codes Used: 128 | Perplexity: 27.67\n",
            "z_q mean: 0.016048 | std: 0.845843\n",
            "[Epoch 85] Train Loss: 0.6764 | Val Loss: 0.1887 | Codes Used: 132 | Perplexity: 27.89\n",
            "z_q mean: 0.017344 | std: 0.847506\n",
            "[Epoch 86] Train Loss: 0.6767 | Val Loss: 0.1896 | Codes Used: 131 | Perplexity: 28.08\n",
            "z_q mean: 0.016938 | std: 0.850587\n",
            "[Epoch 87] Train Loss: 0.6741 | Val Loss: 0.1897 | Codes Used: 131 | Perplexity: 28.23\n",
            "z_q mean: 0.017603 | std: 0.852160\n",
            "[Epoch 88] Train Loss: 0.6728 | Val Loss: 0.1877 | Codes Used: 133 | Perplexity: 28.36\n",
            "z_q mean: 0.016972 | std: 0.855142\n",
            "[Epoch 89] Train Loss: 0.6730 | Val Loss: 0.1870 | Codes Used: 135 | Perplexity: 28.53\n",
            "z_q mean: 0.017768 | std: 0.857344\n",
            "[Epoch 90] Train Loss: 0.6691 | Val Loss: 0.1866 | Codes Used: 135 | Perplexity: 28.62\n",
            "z_q mean: 0.017434 | std: 0.859628\n",
            "[Epoch 91] Train Loss: 0.6707 | Val Loss: 0.1863 | Codes Used: 133 | Perplexity: 28.78\n",
            "z_q mean: 0.018269 | std: 0.862574\n",
            "[Epoch 92] Train Loss: 0.6683 | Val Loss: 0.1861 | Codes Used: 134 | Perplexity: 28.91\n",
            "z_q mean: 0.017950 | std: 0.865251\n",
            "[Epoch 93] Train Loss: 0.6682 | Val Loss: 0.1852 | Codes Used: 133 | Perplexity: 29.08\n",
            "z_q mean: 0.018421 | std: 0.868155\n",
            "[Epoch 94] Train Loss: 0.6671 | Val Loss: 0.1845 | Codes Used: 134 | Perplexity: 29.23\n",
            "z_q mean: 0.018194 | std: 0.870240\n",
            "[Epoch 95] Train Loss: 0.6651 | Val Loss: 0.1862 | Codes Used: 133 | Perplexity: 29.33\n",
            "z_q mean: 0.018718 | std: 0.872653\n",
            "[Epoch 96] Train Loss: 0.6641 | Val Loss: 0.1847 | Codes Used: 132 | Perplexity: 29.48\n",
            "z_q mean: 0.017385 | std: 0.874745\n",
            "[Epoch 97] Train Loss: 0.6629 | Val Loss: 0.1841 | Codes Used: 139 | Perplexity: 29.61\n",
            "z_q mean: 0.018493 | std: 0.877052\n",
            "[Epoch 98] Train Loss: 0.6606 | Val Loss: 0.1842 | Codes Used: 134 | Perplexity: 29.73\n",
            "z_q mean: 0.018158 | std: 0.878851\n",
            "[Epoch 99] Train Loss: 0.6608 | Val Loss: 0.1843 | Codes Used: 137 | Perplexity: 29.83\n",
            "z_q mean: 0.019317 | std: 0.881387\n",
            "[Epoch 100] Train Loss: 0.6597 | Val Loss: 0.1838 | Codes Used: 139 | Perplexity: 29.98\n",
            "z_q mean: 0.019845 | std: 0.883339\n",
            "[Epoch 101] Train Loss: 0.6576 | Val Loss: 0.1832 | Codes Used: 143 | Perplexity: 30.10\n",
            "z_q mean: 0.018730 | std: 0.885278\n",
            "[Epoch 102] Train Loss: 0.6574 | Val Loss: 0.1825 | Codes Used: 138 | Perplexity: 30.26\n",
            "z_q mean: 0.020273 | std: 0.887731\n",
            "[Epoch 103] Train Loss: 0.6559 | Val Loss: 0.1827 | Codes Used: 132 | Perplexity: 30.37\n",
            "z_q mean: 0.020361 | std: 0.889491\n",
            "[Epoch 104] Train Loss: 0.6552 | Val Loss: 0.1814 | Codes Used: 135 | Perplexity: 30.56\n",
            "z_q mean: 0.020124 | std: 0.891761\n",
            "[Epoch 105] Train Loss: 0.6547 | Val Loss: 0.1815 | Codes Used: 132 | Perplexity: 30.73\n",
            "z_q mean: 0.019595 | std: 0.893691\n",
            "[Epoch 106] Train Loss: 0.6527 | Val Loss: 0.1804 | Codes Used: 133 | Perplexity: 30.95\n",
            "z_q mean: 0.019459 | std: 0.895305\n",
            "[Epoch 107] Train Loss: 0.6519 | Val Loss: 0.1807 | Codes Used: 131 | Perplexity: 31.13\n",
            "z_q mean: 0.019236 | std: 0.897048\n",
            "[Epoch 108] Train Loss: 0.6512 | Val Loss: 0.1813 | Codes Used: 128 | Perplexity: 31.40\n",
            "z_q mean: 0.019100 | std: 0.898536\n",
            "[Epoch 109] Train Loss: 0.6499 | Val Loss: 0.1805 | Codes Used: 130 | Perplexity: 31.66\n",
            "z_q mean: 0.021042 | std: 0.899618\n",
            "[Epoch 110] Train Loss: 0.6483 | Val Loss: 0.1795 | Codes Used: 131 | Perplexity: 31.90\n",
            "z_q mean: 0.020287 | std: 0.900522\n",
            "[Epoch 111] Train Loss: 0.6470 | Val Loss: 0.1794 | Codes Used: 134 | Perplexity: 32.16\n",
            "z_q mean: 0.020044 | std: 0.901688\n",
            "[Epoch 112] Train Loss: 0.6469 | Val Loss: 0.1785 | Codes Used: 132 | Perplexity: 32.35\n",
            "z_q mean: 0.020688 | std: 0.902912\n",
            "[Epoch 113] Train Loss: 0.6455 | Val Loss: 0.1782 | Codes Used: 132 | Perplexity: 32.61\n",
            "z_q mean: 0.020249 | std: 0.903951\n",
            "[Epoch 114] Train Loss: 0.6451 | Val Loss: 0.1782 | Codes Used: 135 | Perplexity: 32.82\n",
            "z_q mean: 0.019473 | std: 0.905018\n",
            "[Epoch 115] Train Loss: 0.6434 | Val Loss: 0.1775 | Codes Used: 137 | Perplexity: 33.05\n",
            "z_q mean: 0.020544 | std: 0.905586\n",
            "[Epoch 116] Train Loss: 0.6428 | Val Loss: 0.1777 | Codes Used: 131 | Perplexity: 33.29\n",
            "z_q mean: 0.020062 | std: 0.906707\n",
            "[Epoch 117] Train Loss: 0.6413 | Val Loss: 0.1769 | Codes Used: 134 | Perplexity: 33.45\n",
            "z_q mean: 0.021422 | std: 0.907601\n",
            "[Epoch 118] Train Loss: 0.6396 | Val Loss: 0.1767 | Codes Used: 135 | Perplexity: 33.67\n",
            "z_q mean: 0.019636 | std: 0.908305\n",
            "[Epoch 119] Train Loss: 0.6401 | Val Loss: 0.1763 | Codes Used: 137 | Perplexity: 33.89\n",
            "z_q mean: 0.020627 | std: 0.909634\n",
            "[Epoch 120] Train Loss: 0.6372 | Val Loss: 0.1762 | Codes Used: 139 | Perplexity: 33.98\n",
            "z_q mean: 0.020311 | std: 0.910457\n",
            "[Epoch 121] Train Loss: 0.6371 | Val Loss: 0.1755 | Codes Used: 138 | Perplexity: 34.16\n",
            "z_q mean: 0.020671 | std: 0.911819\n",
            "[Epoch 122] Train Loss: 0.6371 | Val Loss: 0.1758 | Codes Used: 142 | Perplexity: 34.30\n",
            "z_q mean: 0.019171 | std: 0.913130\n",
            "[Epoch 123] Train Loss: 0.6349 | Val Loss: 0.1753 | Codes Used: 142 | Perplexity: 34.38\n",
            "z_q mean: 0.020528 | std: 0.914308\n",
            "[Epoch 124] Train Loss: 0.6356 | Val Loss: 0.1750 | Codes Used: 143 | Perplexity: 34.53\n",
            "z_q mean: 0.019453 | std: 0.916248\n",
            "[Epoch 125] Train Loss: 0.6342 | Val Loss: 0.1746 | Codes Used: 149 | Perplexity: 34.59\n",
            "z_q mean: 0.019775 | std: 0.918002\n",
            "[Epoch 126] Train Loss: 0.6332 | Val Loss: 0.1750 | Codes Used: 148 | Perplexity: 34.69\n",
            "z_q mean: 0.020175 | std: 0.919138\n",
            "[Epoch 127] Train Loss: 0.6320 | Val Loss: 0.1752 | Codes Used: 150 | Perplexity: 34.80\n",
            "z_q mean: 0.020670 | std: 0.920611\n",
            "[Epoch 128] Train Loss: 0.6326 | Val Loss: 0.1745 | Codes Used: 148 | Perplexity: 34.91\n",
            "z_q mean: 0.019655 | std: 0.922589\n",
            "[Epoch 129] Train Loss: 0.6303 | Val Loss: 0.1736 | Codes Used: 149 | Perplexity: 34.99\n",
            "z_q mean: 0.020489 | std: 0.923924\n",
            "[Epoch 130] Train Loss: 0.6310 | Val Loss: 0.1743 | Codes Used: 153 | Perplexity: 35.13\n",
            "z_q mean: 0.020851 | std: 0.925982\n",
            "[Epoch 131] Train Loss: 0.6288 | Val Loss: 0.1736 | Codes Used: 153 | Perplexity: 35.27\n",
            "z_q mean: 0.019781 | std: 0.927379\n",
            "[Epoch 132] Train Loss: 0.6282 | Val Loss: 0.1738 | Codes Used: 146 | Perplexity: 35.41\n",
            "z_q mean: 0.021346 | std: 0.928380\n",
            "[Epoch 133] Train Loss: 0.6278 | Val Loss: 0.1732 | Codes Used: 157 | Perplexity: 35.58\n",
            "z_q mean: 0.020782 | std: 0.930039\n",
            "[Epoch 134] Train Loss: 0.6263 | Val Loss: 0.1732 | Codes Used: 153 | Perplexity: 35.70\n",
            "z_q mean: 0.020083 | std: 0.931170\n",
            "[Epoch 135] Train Loss: 0.6263 | Val Loss: 0.1734 | Codes Used: 151 | Perplexity: 35.80\n",
            "z_q mean: 0.021108 | std: 0.933006\n",
            "[Epoch 136] Train Loss: 0.6257 | Val Loss: 0.1731 | Codes Used: 153 | Perplexity: 35.87\n",
            "z_q mean: 0.020028 | std: 0.934512\n",
            "[Epoch 137] Train Loss: 0.6250 | Val Loss: 0.1730 | Codes Used: 153 | Perplexity: 36.01\n",
            "z_q mean: 0.021508 | std: 0.935747\n",
            "[Epoch 138] Train Loss: 0.6236 | Val Loss: 0.1713 | Codes Used: 151 | Perplexity: 36.09\n",
            "z_q mean: 0.020414 | std: 0.937158\n",
            "[Epoch 139] Train Loss: 0.6233 | Val Loss: 0.1713 | Codes Used: 151 | Perplexity: 36.30\n",
            "z_q mean: 0.020689 | std: 0.938623\n",
            "[Epoch 140] Train Loss: 0.6233 | Val Loss: 0.1715 | Codes Used: 151 | Perplexity: 36.41\n",
            "z_q mean: 0.021068 | std: 0.940127\n",
            "[Epoch 141] Train Loss: 0.6223 | Val Loss: 0.1709 | Codes Used: 146 | Perplexity: 36.62\n",
            "z_q mean: 0.020935 | std: 0.941149\n",
            "[Epoch 142] Train Loss: 0.6213 | Val Loss: 0.1712 | Codes Used: 144 | Perplexity: 36.77\n",
            "z_q mean: 0.021166 | std: 0.942383\n",
            "[Epoch 143] Train Loss: 0.6211 | Val Loss: 0.1702 | Codes Used: 144 | Perplexity: 37.00\n",
            "z_q mean: 0.021155 | std: 0.943710\n",
            "[Epoch 144] Train Loss: 0.6207 | Val Loss: 0.1703 | Codes Used: 143 | Perplexity: 37.18\n",
            "z_q mean: 0.021601 | std: 0.944765\n",
            "[Epoch 145] Train Loss: 0.6191 | Val Loss: 0.1701 | Codes Used: 142 | Perplexity: 37.41\n",
            "z_q mean: 0.021758 | std: 0.945511\n",
            "[Epoch 146] Train Loss: 0.6197 | Val Loss: 0.1701 | Codes Used: 145 | Perplexity: 37.59\n",
            "z_q mean: 0.020894 | std: 0.946988\n",
            "[Epoch 147] Train Loss: 0.6178 | Val Loss: 0.1697 | Codes Used: 148 | Perplexity: 37.79\n",
            "z_q mean: 0.021632 | std: 0.947528\n",
            "[Epoch 148] Train Loss: 0.6176 | Val Loss: 0.1701 | Codes Used: 143 | Perplexity: 37.93\n",
            "z_q mean: 0.022483 | std: 0.948500\n",
            "[Epoch 149] Train Loss: 0.6166 | Val Loss: 0.1697 | Codes Used: 147 | Perplexity: 38.06\n",
            "z_q mean: 0.021227 | std: 0.948906\n",
            "[Epoch 150] Train Loss: 0.6171 | Val Loss: 0.1697 | Codes Used: 144 | Perplexity: 38.26\n",
            "z_q mean: 0.021084 | std: 0.950345\n",
            "[Epoch 151] Train Loss: 0.6157 | Val Loss: 0.1685 | Codes Used: 144 | Perplexity: 38.42\n",
            "z_q mean: 0.021520 | std: 0.951209\n",
            "[Epoch 152] Train Loss: 0.6155 | Val Loss: 0.1690 | Codes Used: 146 | Perplexity: 38.61\n",
            "z_q mean: 0.021616 | std: 0.952021\n",
            "[Epoch 153] Train Loss: 0.6160 | Val Loss: 0.1686 | Codes Used: 142 | Perplexity: 38.73\n",
            "z_q mean: 0.021931 | std: 0.953242\n",
            "[Epoch 154] Train Loss: 0.6144 | Val Loss: 0.1680 | Codes Used: 141 | Perplexity: 38.95\n",
            "z_q mean: 0.022061 | std: 0.953963\n",
            "[Epoch 155] Train Loss: 0.6142 | Val Loss: 0.1682 | Codes Used: 149 | Perplexity: 39.12\n",
            "z_q mean: 0.021829 | std: 0.955203\n",
            "[Epoch 156] Train Loss: 0.6138 | Val Loss: 0.1680 | Codes Used: 146 | Perplexity: 39.29\n",
            "z_q mean: 0.021714 | std: 0.956195\n",
            "[Epoch 157] Train Loss: 0.6140 | Val Loss: 0.1676 | Codes Used: 145 | Perplexity: 39.48\n",
            "z_q mean: 0.021903 | std: 0.957386\n",
            "[Epoch 158] Train Loss: 0.6131 | Val Loss: 0.1670 | Codes Used: 144 | Perplexity: 39.67\n",
            "z_q mean: 0.021976 | std: 0.957906\n",
            "[Epoch 159] Train Loss: 0.6127 | Val Loss: 0.1672 | Codes Used: 146 | Perplexity: 39.91\n",
            "z_q mean: 0.022725 | std: 0.958869\n",
            "[Epoch 160] Train Loss: 0.6117 | Val Loss: 0.1669 | Codes Used: 150 | Perplexity: 40.08\n",
            "z_q mean: 0.022478 | std: 0.959916\n",
            "[Epoch 161] Train Loss: 0.6114 | Val Loss: 0.1667 | Codes Used: 150 | Perplexity: 40.26\n",
            "z_q mean: 0.022398 | std: 0.960656\n",
            "[Epoch 162] Train Loss: 0.6117 | Val Loss: 0.1665 | Codes Used: 148 | Perplexity: 40.44\n",
            "z_q mean: 0.022620 | std: 0.961420\n",
            "[Epoch 163] Train Loss: 0.6103 | Val Loss: 0.1667 | Codes Used: 152 | Perplexity: 40.63\n",
            "z_q mean: 0.022568 | std: 0.962086\n",
            "[Epoch 164] Train Loss: 0.6106 | Val Loss: 0.1660 | Codes Used: 150 | Perplexity: 40.79\n",
            "z_q mean: 0.021957 | std: 0.963452\n",
            "[Epoch 165] Train Loss: 0.6101 | Val Loss: 0.1660 | Codes Used: 153 | Perplexity: 40.95\n",
            "z_q mean: 0.022447 | std: 0.964287\n",
            "[Epoch 166] Train Loss: 0.6095 | Val Loss: 0.1657 | Codes Used: 152 | Perplexity: 41.08\n",
            "z_q mean: 0.022003 | std: 0.965119\n",
            "[Epoch 167] Train Loss: 0.6095 | Val Loss: 0.1656 | Codes Used: 153 | Perplexity: 41.24\n",
            "z_q mean: 0.022216 | std: 0.966298\n",
            "[Epoch 168] Train Loss: 0.6090 | Val Loss: 0.1655 | Codes Used: 156 | Perplexity: 41.37\n",
            "z_q mean: 0.022146 | std: 0.967226\n",
            "[Epoch 169] Train Loss: 0.6089 | Val Loss: 0.1649 | Codes Used: 154 | Perplexity: 41.51\n",
            "z_q mean: 0.022550 | std: 0.968528\n",
            "[Epoch 170] Train Loss: 0.6087 | Val Loss: 0.1648 | Codes Used: 154 | Perplexity: 41.59\n",
            "z_q mean: 0.022077 | std: 0.969714\n",
            "[Epoch 171] Train Loss: 0.6090 | Val Loss: 0.1653 | Codes Used: 158 | Perplexity: 41.74\n",
            "z_q mean: 0.022370 | std: 0.970754\n",
            "[Epoch 172] Train Loss: 0.6079 | Val Loss: 0.1649 | Codes Used: 155 | Perplexity: 41.90\n",
            "z_q mean: 0.022411 | std: 0.971541\n",
            "[Epoch 173] Train Loss: 0.6095 | Val Loss: 0.1643 | Codes Used: 156 | Perplexity: 42.04\n",
            "z_q mean: 0.022692 | std: 0.973467\n",
            "[Epoch 174] Train Loss: 0.6090 | Val Loss: 0.1642 | Codes Used: 154 | Perplexity: 42.23\n",
            "z_q mean: 0.022339 | std: 0.974169\n",
            "[Epoch 175] Train Loss: 0.6085 | Val Loss: 0.1648 | Codes Used: 149 | Perplexity: 42.36\n",
            "z_q mean: 0.022348 | std: 0.975119\n",
            "[Epoch 176] Train Loss: 0.6093 | Val Loss: 0.1640 | Codes Used: 156 | Perplexity: 42.61\n",
            "z_q mean: 0.022458 | std: 0.976260\n",
            "[Epoch 177] Train Loss: 0.6086 | Val Loss: 0.1636 | Codes Used: 151 | Perplexity: 42.79\n",
            "z_q mean: 0.022439 | std: 0.976686\n",
            "[Epoch 178] Train Loss: 0.6082 | Val Loss: 0.1641 | Codes Used: 153 | Perplexity: 43.02\n",
            "z_q mean: 0.022581 | std: 0.977663\n",
            "[Epoch 179] Train Loss: 0.6084 | Val Loss: 0.1644 | Codes Used: 149 | Perplexity: 43.21\n",
            "z_q mean: 0.022514 | std: 0.978069\n",
            "[Epoch 180] Train Loss: 0.6080 | Val Loss: 0.1631 | Codes Used: 154 | Perplexity: 43.43\n",
            "z_q mean: 0.021877 | std: 0.978903\n",
            "[Epoch 181] Train Loss: 0.6079 | Val Loss: 0.1630 | Codes Used: 152 | Perplexity: 43.63\n",
            "z_q mean: 0.022294 | std: 0.979723\n",
            "[Epoch 182] Train Loss: 0.6071 | Val Loss: 0.1628 | Codes Used: 148 | Perplexity: 43.78\n",
            "z_q mean: 0.021748 | std: 0.980134\n",
            "[Epoch 183] Train Loss: 0.6076 | Val Loss: 0.1630 | Codes Used: 152 | Perplexity: 43.95\n",
            "z_q mean: 0.022084 | std: 0.981243\n",
            "[Epoch 184] Train Loss: 0.6067 | Val Loss: 0.1622 | Codes Used: 147 | Perplexity: 44.09\n",
            "z_q mean: 0.022240 | std: 0.981183\n",
            "[Epoch 185] Train Loss: 0.6060 | Val Loss: 0.1624 | Codes Used: 149 | Perplexity: 44.16\n",
            "z_q mean: 0.021652 | std: 0.981627\n",
            "[Epoch 186] Train Loss: 0.6056 | Val Loss: 0.1623 | Codes Used: 153 | Perplexity: 44.33\n",
            "z_q mean: 0.021170 | std: 0.982382\n",
            "[Epoch 187] Train Loss: 0.6052 | Val Loss: 0.1618 | Codes Used: 151 | Perplexity: 44.50\n",
            "z_q mean: 0.021509 | std: 0.983375\n",
            "[Epoch 188] Train Loss: 0.6044 | Val Loss: 0.1616 | Codes Used: 152 | Perplexity: 44.64\n",
            "z_q mean: 0.021485 | std: 0.983444\n",
            "[Epoch 189] Train Loss: 0.6045 | Val Loss: 0.1617 | Codes Used: 156 | Perplexity: 44.83\n",
            "z_q mean: 0.022192 | std: 0.984629\n",
            "[Epoch 190] Train Loss: 0.6025 | Val Loss: 0.1611 | Codes Used: 153 | Perplexity: 44.90\n",
            "z_q mean: 0.021190 | std: 0.983958\n",
            "[Epoch 191] Train Loss: 0.6028 | Val Loss: 0.1611 | Codes Used: 155 | Perplexity: 45.10\n",
            "z_q mean: 0.021589 | std: 0.985057\n",
            "[Epoch 192] Train Loss: 0.6014 | Val Loss: 0.1608 | Codes Used: 157 | Perplexity: 45.25\n",
            "z_q mean: 0.021865 | std: 0.985603\n",
            "[Epoch 193] Train Loss: 0.6013 | Val Loss: 0.1617 | Codes Used: 153 | Perplexity: 45.42\n",
            "z_q mean: 0.021780 | std: 0.986156\n",
            "[Epoch 194] Train Loss: 0.5996 | Val Loss: 0.1603 | Codes Used: 155 | Perplexity: 45.57\n",
            "z_q mean: 0.021909 | std: 0.986167\n",
            "[Epoch 195] Train Loss: 0.5995 | Val Loss: 0.1605 | Codes Used: 154 | Perplexity: 45.72\n",
            "z_q mean: 0.021439 | std: 0.986681\n",
            "[Epoch 196] Train Loss: 0.5980 | Val Loss: 0.1600 | Codes Used: 155 | Perplexity: 45.83\n",
            "z_q mean: 0.020830 | std: 0.986909\n",
            "[Epoch 197] Train Loss: 0.5969 | Val Loss: 0.1604 | Codes Used: 158 | Perplexity: 45.96\n",
            "z_q mean: 0.021080 | std: 0.987075\n",
            "[Epoch 198] Train Loss: 0.5968 | Val Loss: 0.1598 | Codes Used: 157 | Perplexity: 46.08\n",
            "z_q mean: 0.021474 | std: 0.987747\n",
            "[Epoch 199] Train Loss: 0.5951 | Val Loss: 0.1606 | Codes Used: 161 | Perplexity: 46.18\n",
            "z_q mean: 0.020932 | std: 0.987896\n",
            "[Epoch 200] Train Loss: 0.5946 | Val Loss: 0.1595 | Codes Used: 161 | Perplexity: 46.30\n",
            "z_q mean: 0.020738 | std: 0.988571\n",
            "[Epoch 201] Train Loss: 0.5948 | Val Loss: 0.1595 | Codes Used: 166 | Perplexity: 46.34\n",
            "z_q mean: 0.020875 | std: 0.989433\n",
            "[Epoch 202] Train Loss: 0.5935 | Val Loss: 0.1594 | Codes Used: 158 | Perplexity: 46.45\n",
            "z_q mean: 0.021141 | std: 0.989437\n",
            "[Epoch 203] Train Loss: 0.5933 | Val Loss: 0.1597 | Codes Used: 164 | Perplexity: 46.61\n",
            "z_q mean: 0.020983 | std: 0.990286\n",
            "[Epoch 204] Train Loss: 0.5923 | Val Loss: 0.1591 | Codes Used: 164 | Perplexity: 46.67\n",
            "z_q mean: 0.020820 | std: 0.990701\n",
            "[Epoch 205] Train Loss: 0.5922 | Val Loss: 0.1591 | Codes Used: 159 | Perplexity: 46.76\n",
            "z_q mean: 0.019950 | std: 0.991615\n",
            "[Epoch 206] Train Loss: 0.5927 | Val Loss: 0.1590 | Codes Used: 166 | Perplexity: 46.96\n",
            "z_q mean: 0.019639 | std: 0.992380\n",
            "[Epoch 207] Train Loss: 0.5911 | Val Loss: 0.1587 | Codes Used: 162 | Perplexity: 47.03\n",
            "z_q mean: 0.020050 | std: 0.992434\n",
            "[Epoch 208] Train Loss: 0.5906 | Val Loss: 0.1580 | Codes Used: 163 | Perplexity: 47.16\n",
            "z_q mean: 0.019890 | std: 0.993278\n",
            "[Epoch 209] Train Loss: 0.5905 | Val Loss: 0.1582 | Codes Used: 163 | Perplexity: 47.33\n",
            "z_q mean: 0.019991 | std: 0.993740\n",
            "[Epoch 210] Train Loss: 0.5905 | Val Loss: 0.1582 | Codes Used: 169 | Perplexity: 47.46\n",
            "z_q mean: 0.019583 | std: 0.994336\n",
            "[Epoch 211] Train Loss: 0.5909 | Val Loss: 0.1578 | Codes Used: 164 | Perplexity: 47.57\n",
            "z_q mean: 0.019387 | std: 0.995671\n",
            "[Epoch 212] Train Loss: 0.5898 | Val Loss: 0.1583 | Codes Used: 167 | Perplexity: 47.67\n",
            "z_q mean: 0.019230 | std: 0.995667\n",
            "[Epoch 213] Train Loss: 0.5896 | Val Loss: 0.1582 | Codes Used: 164 | Perplexity: 47.80\n",
            "z_q mean: 0.019134 | std: 0.996573\n",
            "[Epoch 214] Train Loss: 0.5893 | Val Loss: 0.1584 | Codes Used: 171 | Perplexity: 47.79\n",
            "z_q mean: 0.018550 | std: 0.997294\n",
            "[Epoch 215] Train Loss: 0.5891 | Val Loss: 0.1584 | Codes Used: 174 | Perplexity: 47.89\n",
            "z_q mean: 0.018639 | std: 0.998162\n",
            "[Epoch 216] Train Loss: 0.5889 | Val Loss: 0.1575 | Codes Used: 169 | Perplexity: 47.91\n",
            "z_q mean: 0.018657 | std: 0.999277\n",
            "[Epoch 217] Train Loss: 0.5880 | Val Loss: 0.1578 | Codes Used: 174 | Perplexity: 47.98\n",
            "z_q mean: 0.018009 | std: 0.999315\n",
            "[Epoch 218] Train Loss: 0.5882 | Val Loss: 0.1572 | Codes Used: 172 | Perplexity: 48.07\n",
            "z_q mean: 0.017945 | std: 1.000621\n",
            "[Epoch 219] Train Loss: 0.5879 | Val Loss: 0.1573 | Codes Used: 171 | Perplexity: 48.11\n",
            "z_q mean: 0.018291 | std: 1.001408\n",
            "[Epoch 220] Train Loss: 0.5878 | Val Loss: 0.1579 | Codes Used: 176 | Perplexity: 48.16\n",
            "z_q mean: 0.017730 | std: 1.002156\n",
            "[Epoch 221] Train Loss: 0.5875 | Val Loss: 0.1571 | Codes Used: 170 | Perplexity: 48.20\n",
            "z_q mean: 0.018228 | std: 1.003103\n",
            "[Epoch 222] Train Loss: 0.5864 | Val Loss: 0.1566 | Codes Used: 172 | Perplexity: 48.21\n",
            "z_q mean: 0.017452 | std: 1.003092\n",
            "[Epoch 223] Train Loss: 0.5875 | Val Loss: 0.1570 | Codes Used: 171 | Perplexity: 48.37\n",
            "z_q mean: 0.017462 | std: 1.004605\n",
            "[Epoch 224] Train Loss: 0.5862 | Val Loss: 0.1566 | Codes Used: 171 | Perplexity: 48.41\n",
            "z_q mean: 0.017136 | std: 1.005178\n",
            "[Epoch 225] Train Loss: 0.5862 | Val Loss: 0.1561 | Codes Used: 174 | Perplexity: 48.49\n",
            "z_q mean: 0.017088 | std: 1.005893\n",
            "[Epoch 226] Train Loss: 0.5855 | Val Loss: 0.1566 | Codes Used: 170 | Perplexity: 48.61\n",
            "z_q mean: 0.017049 | std: 1.006587\n",
            "[Epoch 227] Train Loss: 0.5860 | Val Loss: 0.1565 | Codes Used: 173 | Perplexity: 48.75\n",
            "z_q mean: 0.016868 | std: 1.007310\n",
            "[Epoch 228] Train Loss: 0.5854 | Val Loss: 0.1570 | Codes Used: 172 | Perplexity: 48.91\n",
            "z_q mean: 0.016286 | std: 1.008203\n",
            "[Epoch 229] Train Loss: 0.5856 | Val Loss: 0.1563 | Codes Used: 171 | Perplexity: 49.07\n",
            "z_q mean: 0.016493 | std: 1.008483\n",
            "[Epoch 230] Train Loss: 0.5857 | Val Loss: 0.1567 | Codes Used: 168 | Perplexity: 49.21\n",
            "z_q mean: 0.015818 | std: 1.009313\n",
            "[Epoch 231] Train Loss: 0.5850 | Val Loss: 0.1557 | Codes Used: 174 | Perplexity: 49.36\n",
            "z_q mean: 0.016453 | std: 1.009684\n",
            "[Epoch 232] Train Loss: 0.5850 | Val Loss: 0.1554 | Codes Used: 170 | Perplexity: 49.50\n",
            "z_q mean: 0.015727 | std: 1.009800\n",
            "[Epoch 233] Train Loss: 0.5847 | Val Loss: 0.1554 | Codes Used: 171 | Perplexity: 49.59\n",
            "z_q mean: 0.015278 | std: 1.010456\n",
            "[Epoch 234] Train Loss: 0.5843 | Val Loss: 0.1552 | Codes Used: 171 | Perplexity: 49.66\n",
            "z_q mean: 0.014723 | std: 1.010871\n",
            "[Epoch 235] Train Loss: 0.5838 | Val Loss: 0.1554 | Codes Used: 175 | Perplexity: 49.76\n",
            "z_q mean: 0.015146 | std: 1.011609\n",
            "[Epoch 236] Train Loss: 0.5830 | Val Loss: 0.1552 | Codes Used: 175 | Perplexity: 49.80\n",
            "z_q mean: 0.014398 | std: 1.012015\n",
            "[Epoch 237] Train Loss: 0.5830 | Val Loss: 0.1549 | Codes Used: 175 | Perplexity: 49.88\n",
            "z_q mean: 0.014961 | std: 1.012746\n",
            "[Epoch 238] Train Loss: 0.5830 | Val Loss: 0.1554 | Codes Used: 174 | Perplexity: 49.93\n",
            "z_q mean: 0.015056 | std: 1.013177\n",
            "[Epoch 239] Train Loss: 0.5823 | Val Loss: 0.1551 | Codes Used: 173 | Perplexity: 50.00\n",
            "z_q mean: 0.015029 | std: 1.013830\n",
            "[Epoch 240] Train Loss: 0.5825 | Val Loss: 0.1548 | Codes Used: 177 | Perplexity: 50.08\n",
            "z_q mean: 0.014714 | std: 1.014460\n",
            "[Epoch 241] Train Loss: 0.5816 | Val Loss: 0.1552 | Codes Used: 172 | Perplexity: 50.13\n",
            "z_q mean: 0.014292 | std: 1.015050\n",
            "[Epoch 242] Train Loss: 0.5811 | Val Loss: 0.1546 | Codes Used: 175 | Perplexity: 50.19\n",
            "z_q mean: 0.014743 | std: 1.015324\n",
            "[Epoch 243] Train Loss: 0.5813 | Val Loss: 0.1549 | Codes Used: 172 | Perplexity: 50.26\n",
            "z_q mean: 0.013578 | std: 1.016225\n",
            "[Epoch 244] Train Loss: 0.5799 | Val Loss: 0.1552 | Codes Used: 175 | Perplexity: 50.29\n",
            "z_q mean: 0.013678 | std: 1.016179\n",
            "[Epoch 245] Train Loss: 0.5805 | Val Loss: 0.1544 | Codes Used: 179 | Perplexity: 50.40\n",
            "z_q mean: 0.013992 | std: 1.017428\n",
            "[Epoch 246] Train Loss: 0.5796 | Val Loss: 0.1547 | Codes Used: 172 | Perplexity: 50.41\n",
            "z_q mean: 0.013308 | std: 1.017698\n",
            "[Epoch 247] Train Loss: 0.5796 | Val Loss: 0.1544 | Codes Used: 176 | Perplexity: 50.50\n",
            "z_q mean: 0.013426 | std: 1.018681\n",
            "[Epoch 248] Train Loss: 0.5787 | Val Loss: 0.1552 | Codes Used: 175 | Perplexity: 50.57\n",
            "z_q mean: 0.013563 | std: 1.018556\n",
            "[Epoch 249] Train Loss: 0.5783 | Val Loss: 0.1542 | Codes Used: 176 | Perplexity: 50.55\n",
            "z_q mean: 0.013233 | std: 1.019210\n",
            "[Epoch 250] Train Loss: 0.5782 | Val Loss: 0.1542 | Codes Used: 180 | Perplexity: 50.60\n",
            "z_q mean: 0.012837 | std: 1.019980\n",
            "[Epoch 251] Train Loss: 0.5786 | Val Loss: 0.1545 | Codes Used: 177 | Perplexity: 50.68\n",
            "z_q mean: 0.012676 | std: 1.021034\n",
            "[Epoch 252] Train Loss: 0.5781 | Val Loss: 0.1546 | Codes Used: 176 | Perplexity: 50.67\n",
            "z_q mean: 0.012755 | std: 1.021304\n",
            "[Epoch 253] Train Loss: 0.5778 | Val Loss: 0.1541 | Codes Used: 179 | Perplexity: 50.65\n",
            "z_q mean: 0.012975 | std: 1.021986\n",
            "[Epoch 254] Train Loss: 0.5771 | Val Loss: 0.1541 | Codes Used: 175 | Perplexity: 50.75\n",
            "z_q mean: 0.012753 | std: 1.022396\n",
            "[Epoch 255] Train Loss: 0.5774 | Val Loss: 0.1540 | Codes Used: 177 | Perplexity: 50.77\n",
            "z_q mean: 0.012353 | std: 1.023230\n",
            "[Epoch 256] Train Loss: 0.5771 | Val Loss: 0.1540 | Codes Used: 176 | Perplexity: 50.88\n",
            "z_q mean: 0.012449 | std: 1.023417\n",
            "[Epoch 257] Train Loss: 0.5770 | Val Loss: 0.1539 | Codes Used: 175 | Perplexity: 50.90\n",
            "z_q mean: 0.012533 | std: 1.023843\n",
            "[Epoch 258] Train Loss: 0.5768 | Val Loss: 0.1538 | Codes Used: 181 | Perplexity: 50.97\n",
            "z_q mean: 0.012501 | std: 1.024800\n",
            "[Epoch 259] Train Loss: 0.5768 | Val Loss: 0.1535 | Codes Used: 174 | Perplexity: 51.05\n",
            "z_q mean: 0.011771 | std: 1.024958\n",
            "[Epoch 260] Train Loss: 0.5767 | Val Loss: 0.1537 | Codes Used: 176 | Perplexity: 51.20\n",
            "z_q mean: 0.011615 | std: 1.025693\n",
            "[Epoch 261] Train Loss: 0.5754 | Val Loss: 0.1535 | Codes Used: 176 | Perplexity: 51.26\n",
            "z_q mean: 0.011373 | std: 1.025773\n",
            "[Epoch 262] Train Loss: 0.5764 | Val Loss: 0.1535 | Codes Used: 180 | Perplexity: 51.37\n",
            "z_q mean: 0.011510 | std: 1.026555\n",
            "[Epoch 263] Train Loss: 0.5758 | Val Loss: 0.1539 | Codes Used: 175 | Perplexity: 51.42\n",
            "z_q mean: 0.010872 | std: 1.026935\n",
            "[Epoch 264] Train Loss: 0.5754 | Val Loss: 0.1533 | Codes Used: 176 | Perplexity: 51.44\n",
            "z_q mean: 0.010924 | std: 1.027118\n",
            "[Epoch 265] Train Loss: 0.5748 | Val Loss: 0.1535 | Codes Used: 174 | Perplexity: 51.58\n",
            "z_q mean: 0.010552 | std: 1.027744\n",
            "[Epoch 266] Train Loss: 0.5743 | Val Loss: 0.1534 | Codes Used: 174 | Perplexity: 51.68\n",
            "z_q mean: 0.010624 | std: 1.028002\n",
            "[Epoch 267] Train Loss: 0.5747 | Val Loss: 0.1536 | Codes Used: 173 | Perplexity: 51.76\n",
            "z_q mean: 0.010676 | std: 1.028704\n",
            "[Epoch 268] Train Loss: 0.5745 | Val Loss: 0.1534 | Codes Used: 174 | Perplexity: 51.83\n",
            "z_q mean: 0.010552 | std: 1.029164\n",
            "[Epoch 269] Train Loss: 0.5735 | Val Loss: 0.1528 | Codes Used: 176 | Perplexity: 51.92\n",
            "z_q mean: 0.009966 | std: 1.029139\n",
            "[Epoch 270] Train Loss: 0.5733 | Val Loss: 0.1529 | Codes Used: 176 | Perplexity: 52.05\n",
            "z_q mean: 0.010155 | std: 1.029651\n",
            "[Epoch 271] Train Loss: 0.5732 | Val Loss: 0.1529 | Codes Used: 177 | Perplexity: 52.13\n",
            "z_q mean: 0.009739 | std: 1.029852\n",
            "[Epoch 272] Train Loss: 0.5728 | Val Loss: 0.1531 | Codes Used: 174 | Perplexity: 52.26\n",
            "z_q mean: 0.009746 | std: 1.030118\n",
            "[Epoch 273] Train Loss: 0.5727 | Val Loss: 0.1528 | Codes Used: 172 | Perplexity: 52.33\n",
            "z_q mean: 0.010277 | std: 1.030808\n",
            "[Epoch 274] Train Loss: 0.5721 | Val Loss: 0.1527 | Codes Used: 175 | Perplexity: 52.43\n",
            "z_q mean: 0.010390 | std: 1.030927\n",
            "[Epoch 275] Train Loss: 0.5724 | Val Loss: 0.1532 | Codes Used: 172 | Perplexity: 52.52\n",
            "z_q mean: 0.010063 | std: 1.031156\n",
            "[Epoch 276] Train Loss: 0.5720 | Val Loss: 0.1525 | Codes Used: 177 | Perplexity: 52.61\n",
            "z_q mean: 0.009295 | std: 1.031719\n",
            "[Epoch 277] Train Loss: 0.5715 | Val Loss: 0.1523 | Codes Used: 175 | Perplexity: 52.64\n",
            "z_q mean: 0.009420 | std: 1.031808\n",
            "[Epoch 278] Train Loss: 0.5711 | Val Loss: 0.1524 | Codes Used: 173 | Perplexity: 52.78\n",
            "z_q mean: 0.009801 | std: 1.032439\n",
            "[Epoch 279] Train Loss: 0.5717 | Val Loss: 0.1520 | Codes Used: 174 | Perplexity: 52.86\n",
            "z_q mean: 0.009746 | std: 1.033023\n",
            "[Epoch 280] Train Loss: 0.5710 | Val Loss: 0.1523 | Codes Used: 172 | Perplexity: 53.00\n",
            "z_q mean: 0.009146 | std: 1.032768\n",
            "[Epoch 281] Train Loss: 0.5712 | Val Loss: 0.1528 | Codes Used: 177 | Perplexity: 53.18\n",
            "z_q mean: 0.009207 | std: 1.033436\n",
            "[Epoch 282] Train Loss: 0.5711 | Val Loss: 0.1525 | Codes Used: 177 | Perplexity: 53.24\n",
            "z_q mean: 0.009261 | std: 1.033928\n",
            "[Epoch 283] Train Loss: 0.5703 | Val Loss: 0.1524 | Codes Used: 176 | Perplexity: 53.37\n",
            "z_q mean: 0.009330 | std: 1.033885\n",
            "[Epoch 284] Train Loss: 0.5702 | Val Loss: 0.1522 | Codes Used: 176 | Perplexity: 53.47\n",
            "z_q mean: 0.009450 | std: 1.034061\n",
            "[Epoch 285] Train Loss: 0.5700 | Val Loss: 0.1522 | Codes Used: 178 | Perplexity: 53.60\n",
            "z_q mean: 0.009181 | std: 1.034737\n",
            "[Epoch 286] Train Loss: 0.5693 | Val Loss: 0.1522 | Codes Used: 178 | Perplexity: 53.67\n",
            "z_q mean: 0.009336 | std: 1.034503\n",
            "[Epoch 287] Train Loss: 0.5695 | Val Loss: 0.1519 | Codes Used: 178 | Perplexity: 53.77\n",
            "z_q mean: 0.008813 | std: 1.035507\n",
            "[Epoch 288] Train Loss: 0.5689 | Val Loss: 0.1517 | Codes Used: 180 | Perplexity: 53.90\n",
            "z_q mean: 0.009093 | std: 1.035429\n",
            "[Epoch 289] Train Loss: 0.5688 | Val Loss: 0.1517 | Codes Used: 179 | Perplexity: 53.96\n",
            "z_q mean: 0.008972 | std: 1.035768\n",
            "[Epoch 290] Train Loss: 0.5684 | Val Loss: 0.1518 | Codes Used: 178 | Perplexity: 54.11\n",
            "z_q mean: 0.008646 | std: 1.035987\n",
            "[Epoch 291] Train Loss: 0.5689 | Val Loss: 0.1517 | Codes Used: 175 | Perplexity: 54.20\n",
            "z_q mean: 0.008762 | std: 1.036437\n",
            "[Epoch 292] Train Loss: 0.5687 | Val Loss: 0.1521 | Codes Used: 173 | Perplexity: 54.31\n",
            "z_q mean: 0.008101 | std: 1.037037\n",
            "[Epoch 293] Train Loss: 0.5688 | Val Loss: 0.1516 | Codes Used: 176 | Perplexity: 54.40\n",
            "z_q mean: 0.008858 | std: 1.037534\n",
            "[Epoch 294] Train Loss: 0.5679 | Val Loss: 0.1520 | Codes Used: 176 | Perplexity: 54.50\n",
            "z_q mean: 0.008600 | std: 1.037522\n",
            "[Epoch 295] Train Loss: 0.5682 | Val Loss: 0.1514 | Codes Used: 173 | Perplexity: 54.60\n",
            "z_q mean: 0.008607 | std: 1.037960\n",
            "[Epoch 296] Train Loss: 0.5685 | Val Loss: 0.1512 | Codes Used: 179 | Perplexity: 54.79\n",
            "z_q mean: 0.008499 | std: 1.038731\n",
            "[Epoch 297] Train Loss: 0.5681 | Val Loss: 0.1514 | Codes Used: 183 | Perplexity: 54.89\n",
            "z_q mean: 0.008700 | std: 1.038769\n",
            "[Epoch 298] Train Loss: 0.5683 | Val Loss: 0.1513 | Codes Used: 177 | Perplexity: 55.05\n",
            "z_q mean: 0.008110 | std: 1.039209\n",
            "[Epoch 299] Train Loss: 0.5674 | Val Loss: 0.1510 | Codes Used: 181 | Perplexity: 55.17\n",
            "z_q mean: 0.008493 | std: 1.039176\n",
            "[Epoch 300] Train Loss: 0.5674 | Val Loss: 0.1512 | Codes Used: 184 | Perplexity: 55.34\n",
            "z_q mean: 0.008734 | std: 1.039499\n",
            "[Epoch 301] Train Loss: 0.5672 | Val Loss: 0.1516 | Codes Used: 183 | Perplexity: 55.46\n",
            "z_q mean: 0.007826 | std: 1.039613\n",
            "[Epoch 302] Train Loss: 0.5657 | Val Loss: 0.1512 | Codes Used: 184 | Perplexity: 55.59\n",
            "z_q mean: 0.007984 | std: 1.039342\n",
            "[Epoch 303] Train Loss: 0.5668 | Val Loss: 0.1506 | Codes Used: 183 | Perplexity: 55.75\n",
            "z_q mean: 0.008296 | std: 1.040343\n",
            "[Epoch 304] Train Loss: 0.5661 | Val Loss: 0.1514 | Codes Used: 187 | Perplexity: 55.86\n",
            "z_q mean: 0.008354 | std: 1.040043\n",
            "[Epoch 305] Train Loss: 0.5650 | Val Loss: 0.1508 | Codes Used: 188 | Perplexity: 55.92\n",
            "z_q mean: 0.008293 | std: 1.040126\n",
            "[Epoch 306] Train Loss: 0.5648 | Val Loss: 0.1506 | Codes Used: 188 | Perplexity: 56.02\n",
            "z_q mean: 0.008112 | std: 1.040257\n",
            "[Epoch 307] Train Loss: 0.5648 | Val Loss: 0.1510 | Codes Used: 190 | Perplexity: 56.19\n",
            "z_q mean: 0.008198 | std: 1.040341\n",
            "[Epoch 308] Train Loss: 0.5641 | Val Loss: 0.1505 | Codes Used: 190 | Perplexity: 56.30\n",
            "z_q mean: 0.008652 | std: 1.040063\n",
            "[Epoch 309] Train Loss: 0.5645 | Val Loss: 0.1509 | Codes Used: 189 | Perplexity: 56.33\n",
            "z_q mean: 0.008080 | std: 1.040939\n",
            "[Epoch 310] Train Loss: 0.5642 | Val Loss: 0.1505 | Codes Used: 187 | Perplexity: 56.42\n",
            "z_q mean: 0.009288 | std: 1.041212\n",
            "[Epoch 311] Train Loss: 0.5636 | Val Loss: 0.1504 | Codes Used: 191 | Perplexity: 56.42\n",
            "z_q mean: 0.008710 | std: 1.040795\n",
            "[Epoch 312] Train Loss: 0.5631 | Val Loss: 0.1505 | Codes Used: 197 | Perplexity: 56.46\n",
            "z_q mean: 0.009003 | std: 1.041596\n",
            "[Epoch 313] Train Loss: 0.5622 | Val Loss: 0.1506 | Codes Used: 189 | Perplexity: 56.51\n",
            "z_q mean: 0.007784 | std: 1.041376\n",
            "[Epoch 314] Train Loss: 0.5624 | Val Loss: 0.1503 | Codes Used: 193 | Perplexity: 56.61\n",
            "z_q mean: 0.008678 | std: 1.042168\n",
            "[Epoch 315] Train Loss: 0.5620 | Val Loss: 0.1505 | Codes Used: 195 | Perplexity: 56.64\n",
            "z_q mean: 0.009200 | std: 1.042002\n",
            "[Epoch 316] Train Loss: 0.5624 | Val Loss: 0.1499 | Codes Used: 191 | Perplexity: 56.68\n",
            "z_q mean: 0.008722 | std: 1.042791\n",
            "[Epoch 317] Train Loss: 0.5624 | Val Loss: 0.1501 | Codes Used: 191 | Perplexity: 56.80\n",
            "z_q mean: 0.009448 | std: 1.043110\n",
            "[Epoch 318] Train Loss: 0.5614 | Val Loss: 0.1502 | Codes Used: 194 | Perplexity: 56.81\n",
            "z_q mean: 0.009138 | std: 1.042761\n",
            "[Epoch 319] Train Loss: 0.5611 | Val Loss: 0.1504 | Codes Used: 193 | Perplexity: 56.85\n",
            "z_q mean: 0.009110 | std: 1.043267\n",
            "[Epoch 320] Train Loss: 0.5611 | Val Loss: 0.1499 | Codes Used: 196 | Perplexity: 56.92\n",
            "z_q mean: 0.008678 | std: 1.043955\n",
            "[Epoch 321] Train Loss: 0.5611 | Val Loss: 0.1499 | Codes Used: 193 | Perplexity: 56.96\n",
            "z_q mean: 0.008921 | std: 1.044129\n",
            "[Epoch 322] Train Loss: 0.5615 | Val Loss: 0.1506 | Codes Used: 193 | Perplexity: 57.09\n",
            "z_q mean: 0.010001 | std: 1.044799\n",
            "[Epoch 323] Train Loss: 0.5599 | Val Loss: 0.1495 | Codes Used: 197 | Perplexity: 57.13\n",
            "z_q mean: 0.010799 | std: 1.044513\n",
            "[Epoch 324] Train Loss: 0.5605 | Val Loss: 0.1498 | Codes Used: 193 | Perplexity: 57.22\n",
            "z_q mean: 0.009991 | std: 1.045294\n",
            "[Epoch 325] Train Loss: 0.5593 | Val Loss: 0.1501 | Codes Used: 194 | Perplexity: 57.33\n",
            "z_q mean: 0.010336 | std: 1.044922\n",
            "[Epoch 326] Train Loss: 0.5601 | Val Loss: 0.1498 | Codes Used: 193 | Perplexity: 57.43\n",
            "z_q mean: 0.009527 | std: 1.046029\n",
            "[Epoch 327] Train Loss: 0.5583 | Val Loss: 0.1497 | Codes Used: 196 | Perplexity: 57.43\n",
            "z_q mean: 0.009603 | std: 1.045632\n",
            "[Epoch 328] Train Loss: 0.5587 | Val Loss: 0.1499 | Codes Used: 195 | Perplexity: 57.54\n",
            "z_q mean: 0.012097 | std: 1.046261\n",
            "[Epoch 329] Train Loss: 0.5588 | Val Loss: 0.1501 | Codes Used: 194 | Perplexity: 57.55\n",
            "z_q mean: 0.010426 | std: 1.046759\n",
            "[Epoch 330] Train Loss: 0.5577 | Val Loss: 0.1494 | Codes Used: 193 | Perplexity: 57.63\n",
            "z_q mean: 0.010668 | std: 1.046921\n",
            "[Epoch 331] Train Loss: 0.5574 | Val Loss: 0.1497 | Codes Used: 191 | Perplexity: 57.64\n",
            "z_q mean: 0.010373 | std: 1.046798\n",
            "[Epoch 332] Train Loss: 0.5571 | Val Loss: 0.1495 | Codes Used: 196 | Perplexity: 57.76\n",
            "z_q mean: 0.012116 | std: 1.047226\n",
            "[Epoch 333] Train Loss: 0.5567 | Val Loss: 0.1494 | Codes Used: 195 | Perplexity: 57.84\n",
            "z_q mean: 0.009646 | std: 1.047362\n",
            "[Epoch 334] Train Loss: 0.5570 | Val Loss: 0.1496 | Codes Used: 194 | Perplexity: 57.89\n",
            "z_q mean: 0.010057 | std: 1.047698\n",
            "[Epoch 335] Train Loss: 0.5572 | Val Loss: 0.1492 | Codes Used: 197 | Perplexity: 57.95\n",
            "z_q mean: 0.010045 | std: 1.048512\n",
            "[Epoch 336] Train Loss: 0.5561 | Val Loss: 0.1493 | Codes Used: 194 | Perplexity: 57.99\n",
            "z_q mean: 0.011279 | std: 1.048228\n",
            "[Epoch 337] Train Loss: 0.5555 | Val Loss: 0.1493 | Codes Used: 200 | Perplexity: 58.07\n",
            "z_q mean: 0.010434 | std: 1.048287\n",
            "[Epoch 338] Train Loss: 0.5563 | Val Loss: 0.1494 | Codes Used: 198 | Perplexity: 58.11\n",
            "z_q mean: 0.013084 | std: 1.049012\n",
            "[Epoch 339] Train Loss: 0.5555 | Val Loss: 0.1496 | Codes Used: 198 | Perplexity: 58.11\n",
            "z_q mean: 0.011694 | std: 1.049109\n",
            "[Epoch 340] Train Loss: 0.5555 | Val Loss: 0.1488 | Codes Used: 194 | Perplexity: 58.14\n",
            "z_q mean: 0.012866 | std: 1.049667\n",
            "[Epoch 341] Train Loss: 0.5553 | Val Loss: 0.1490 | Codes Used: 192 | Perplexity: 58.24\n",
            "z_q mean: 0.012824 | std: 1.049879\n",
            "[Epoch 342] Train Loss: 0.5552 | Val Loss: 0.1489 | Codes Used: 195 | Perplexity: 58.29\n",
            "z_q mean: 0.012603 | std: 1.050110\n",
            "[Epoch 343] Train Loss: 0.5551 | Val Loss: 0.1495 | Codes Used: 195 | Perplexity: 58.30\n",
            "z_q mean: 0.013269 | std: 1.050234\n",
            "[Epoch 344] Train Loss: 0.5544 | Val Loss: 0.1495 | Codes Used: 194 | Perplexity: 58.35\n",
            "z_q mean: 0.012956 | std: 1.050793\n",
            "[Epoch 345] Train Loss: 0.5539 | Val Loss: 0.1490 | Codes Used: 197 | Perplexity: 58.35\n",
            "z_q mean: 0.011495 | std: 1.050832\n",
            "[Epoch 346] Train Loss: 0.5539 | Val Loss: 0.1492 | Codes Used: 194 | Perplexity: 58.41\n",
            "z_q mean: 0.013122 | std: 1.051168\n",
            "[Epoch 347] Train Loss: 0.5542 | Val Loss: 0.1489 | Codes Used: 197 | Perplexity: 58.45\n",
            "z_q mean: 0.013047 | std: 1.052160\n",
            "[Epoch 348] Train Loss: 0.5539 | Val Loss: 0.1488 | Codes Used: 198 | Perplexity: 58.46\n",
            "z_q mean: 0.013043 | std: 1.052361\n",
            "[Epoch 349] Train Loss: 0.5533 | Val Loss: 0.1486 | Codes Used: 193 | Perplexity: 58.58\n",
            "z_q mean: 0.013151 | std: 1.052148\n",
            "[Epoch 350] Train Loss: 0.5534 | Val Loss: 0.1493 | Codes Used: 194 | Perplexity: 58.61\n",
            "z_q mean: 0.013735 | std: 1.052568\n",
            "[Epoch 351] Train Loss: 0.5538 | Val Loss: 0.1482 | Codes Used: 198 | Perplexity: 58.61\n",
            "z_q mean: 0.013286 | std: 1.053207\n"
          ]
        }
      ],
      "source": [
        "model = VQVAE(beta=0.45)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train_vqvae(model, train_loader, val_loader, optimizer, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b45aa5",
      "metadata": {
        "id": "f2b45aa5"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model pt files/vqvae_model.pth\")\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/vqvae_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd73459",
      "metadata": {
        "id": "8cd73459"
      },
      "outputs": [],
      "source": [
        "# Define the model again (same config as used during training)\n",
        "model = VQVAE(in_channels=1, embedding_dim=128, num_embeddings=512, beta=0.5)\n",
        "model.load_state_dict(torch.load(\"model pt files/vqvae_model.pth\"))\n",
        "model.eval()\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Get a batch from the validation loader\n",
        "device = next(model.parameters()).device\n",
        "val_iter = iter(test_loader)\n",
        "batch = next(val_iter).to(device)\n",
        "\n",
        "# Run the model\n",
        "with torch.no_grad():\n",
        "    output = model(batch)\n",
        "    recon = output[\"recon_x\"]\n",
        "\n",
        "# Plot original and reconstructed images\n",
        "n = min(6, batch.size(0))  # number of samples to show\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i in range(n):\n",
        "    # Original\n",
        "    plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(batch[i][0].cpu().numpy(), cmap='viridis', aspect='auto')\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Reconstructed\n",
        "    plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(recon[i][0].cpu().numpy(), cmap='viridis', aspect='auto')\n",
        "    plt.title(\"Reconstructed\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"output_images/ReconstructionExamples.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2905614",
      "metadata": {
        "id": "e2905614"
      },
      "source": [
        "# Average Latent Space Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d83c5b",
      "metadata": {
        "id": "a3d83c5b"
      },
      "outputs": [],
      "source": [
        "def validate_chunks(chunks):\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if chunk['exp_no'].nunique() != 1 or chunk['activity'].nunique() != 1:\n",
        "            raise ValueError(f\"Chunk {i} contains multiple exp_no or activity values.\")\n",
        "    print(\"All chunks have consistent 'exp_no' and 'activity'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c5e956",
      "metadata": {
        "id": "b4c5e956"
      },
      "outputs": [],
      "source": [
        "def sample_chunks_by_activity(chunks, n_per_activity=5, seed=2555304):\n",
        "    random.seed(seed)\n",
        "    activity_map = defaultdict(list)\n",
        "    for chunk in chunks:\n",
        "        activity = chunk['activity'].iloc[0]\n",
        "        activity_map[activity].append(chunk)\n",
        "\n",
        "    sampled_chunks = []\n",
        "    for activity, chunk_list in activity_map.items():\n",
        "        sampled = random.sample(chunk_list, min(n_per_activity, len(chunk_list)))\n",
        "        sampled_chunks.extend(sampled)\n",
        "    return sampled_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a184df0c",
      "metadata": {
        "id": "a184df0c"
      },
      "outputs": [],
      "source": [
        "def chunk_to_image(chunk, channel='PWR_ch1'):\n",
        "    # Safely reshape each (200,) array to (200, 1), then stack horizontally\n",
        "    images = [arr.reshape(200, 1) for arr in chunk[channel].values]\n",
        "    return np.hstack(images)  # Result: (200, 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d6c90ae",
      "metadata": {
        "id": "2d6c90ae"
      },
      "outputs": [],
      "source": [
        "def plot_chunks_as_images(chunks, cmap='inferno'):\n",
        "    activity_map = defaultdict(list)\n",
        "    for chunk in chunks:\n",
        "        activity = chunk['activity'].iloc[0]\n",
        "        activity_map[activity].append(chunk)\n",
        "\n",
        "    for activity, chunk_list in activity_map.items():\n",
        "        n = len(chunk_list)\n",
        "        cols = 5\n",
        "        rows = int(np.ceil(n / cols))\n",
        "        plt.figure(figsize=(4 * cols, 3 * rows))\n",
        "\n",
        "        for i, chunk in enumerate(chunk_list):\n",
        "            img = chunk_to_image(chunk)\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img, aspect='auto', cmap=cmap, origin='lower')\n",
        "            plt.title(f\"{activity} #{i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.suptitle(f\"Activity: {activity}\", fontsize=16, y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"output_images/Sampled Images.png\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e9793a",
      "metadata": {
        "id": "e5e9793a"
      },
      "outputs": [],
      "source": [
        "# Validate all chunks\n",
        "validate_chunks(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75fc07ea",
      "metadata": {
        "id": "75fc07ea"
      },
      "outputs": [],
      "source": [
        "# Sample 5 chunks per activity\n",
        "sampled_chunks = sample_chunks_by_activity(chunks, n_per_activity=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76623793",
      "metadata": {
        "id": "76623793"
      },
      "outputs": [],
      "source": [
        "# Plot as images\n",
        "plot_chunks_as_images(sampled_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df74eb08",
      "metadata": {
        "id": "df74eb08"
      },
      "outputs": [],
      "source": [
        "def count_chunks_by_activity(chunks):\n",
        "    activity_counts = Counter(chunk['activity'].iloc[0] for chunk in chunks)\n",
        "    return activity_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee466c68",
      "metadata": {
        "id": "ee466c68"
      },
      "outputs": [],
      "source": [
        "activity_counts = count_chunks_by_activity(chunks)\n",
        "\n",
        "for activity, count in activity_counts.items():\n",
        "    print(f\"{activity}: {count} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bc9de01",
      "metadata": {
        "id": "7bc9de01"
      },
      "outputs": [],
      "source": [
        "def filter_out_class(chunks, class_to_remove='noactivity'):\n",
        "    return [chunk for chunk in chunks if chunk['activity'].iloc[0] != class_to_remove]\n",
        "\n",
        "chunks = filter_out_class(chunks, class_to_remove='noactivity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd5e9eab",
      "metadata": {
        "id": "dd5e9eab"
      },
      "outputs": [],
      "source": [
        "# Build matrix of chunk-level features\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    # Convert each list in 'PWR_ch1' column into a single vector\n",
        "    # e.g., mean over rows, or flatten if consistent shape\n",
        "    power_data = np.stack(chunk['PWR_ch1'].values)  # shape: (chunk_size, signal_len)\n",
        "    pooled = power_data.mean(axis=0)                # average across rows\n",
        "    features.append(pooled)\n",
        "\n",
        "    activity = chunk['activity'].iloc[0]\n",
        "    labels.append(activity)\n",
        "\n",
        "X = np.stack(features)\n",
        "y = np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c064315a",
      "metadata": {
        "id": "c064315a"
      },
      "outputs": [],
      "source": [
        "# Oversample\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=2555304)\n",
        "X, y = ros.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448cfab6",
      "metadata": {
        "id": "448cfab6"
      },
      "outputs": [],
      "source": [
        "# Optional: PCA first to reduce dimensionality before t-SNE\n",
        "pca = PCA(n_components=50)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=2555304, perplexity=30, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(X_pca)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9011d7",
      "metadata": {
        "id": "2a9011d7"
      },
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_components=2, random_state=2555304)\n",
        "X_umap = umap_model.fit_transform(X_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150f8bc9",
      "metadata": {
        "id": "150f8bc9"
      },
      "outputs": [],
      "source": [
        "def plot_embedding(X_embedded, labels, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=labels, palette='tab10', s=20)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Dim 1\")\n",
        "    plt.ylabel(\"Dim 2\")\n",
        "    plt.legend(title=\"Activity\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"output_images/{title}.png\")\n",
        "    plt.show()\n",
        "\n",
        "# t-SNE plot\n",
        "plot_embedding(X_tsne, y, \"t-SNE of Chunks (pooled PWR_ch1)\")\n",
        "\n",
        "# UMAP plot\n",
        "plot_embedding(X_umap, y, \"UMAP of Chunks (pooled PWR_ch1)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1533cee",
      "metadata": {
        "id": "b1533cee"
      },
      "outputs": [],
      "source": [
        "chunk_tensors = []\n",
        "chunk_labels = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    # shape: (chunk_size, signal_length)\n",
        "    arr = np.stack(chunk['PWR_ch1'].values)\n",
        "    tensor = torch.tensor(arr, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)\n",
        "    chunk_tensors.append(tensor)\n",
        "    chunk_labels.append(chunk['activity'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2040abaf",
      "metadata": {
        "id": "2040abaf"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "ze_list = []\n",
        "zq_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for tensor in chunk_tensors:\n",
        "        tensor = tensor.to(device)\n",
        "        z_e = model.encoder(tensor)            # (1, C, H', W')\n",
        "        z_q, *_ = model.quantizer(z_e)         # (1, C, H', W')\n",
        "\n",
        "        ze_flat = z_e.squeeze(0).cpu().numpy().reshape(-1)  # flatten to 1D\n",
        "        zq_flat = z_q.squeeze(0).cpu().numpy().reshape(-1)\n",
        "\n",
        "        ze_list.append(ze_flat)\n",
        "        zq_list.append(zq_flat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b456dd",
      "metadata": {
        "id": "23b456dd"
      },
      "outputs": [],
      "source": [
        "ze_matrix = np.stack(ze_list)\n",
        "zq_matrix = np.stack(zq_list)\n",
        "\n",
        "# PCA to reduce noise\n",
        "print(\"Running PCA...\")\n",
        "pca = PCA(n_components=30)\n",
        "ze_pca = pca.fit_transform(ze_matrix)\n",
        "zq_pca = pca.fit_transform(zq_matrix)\n",
        "\n",
        "# t-SNE\n",
        "print(\"Running t-SNE...\")\n",
        "tsne = TSNE(n_components=2, random_state=2555304, perplexity=30)\n",
        "ze_tsne = tsne.fit_transform(ze_pca)\n",
        "zq_tsne = tsne.fit_transform(zq_pca)\n",
        "\n",
        "# UMAP\n",
        "print(\"Running UMAP...\")\n",
        "umap_model = umap.UMAP(n_components=2, random_state=2555304)\n",
        "ze_umap = umap_model.fit_transform(ze_pca)\n",
        "zq_umap = umap_model.fit_transform(zq_pca)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "210a3e8c",
      "metadata": {
        "id": "210a3e8c"
      },
      "outputs": [],
      "source": [
        "def plot_2d(embedding, labels, title):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=labels, palette='tab10', s=20)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Dim 1\")\n",
        "    plt.ylabel(\"Dim 2\")\n",
        "    plt.legend(title=\"Activity\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"output_images/{title}.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot t-SNE\n",
        "plot_2d(ze_tsne, chunk_labels, \"t-SNE of z_e\")\n",
        "plot_2d(zq_tsne, chunk_labels, \"t-SNE of z_q\")\n",
        "\n",
        "# Plot UMAP\n",
        "plot_2d(ze_umap, chunk_labels, \"UMAP of z_e\")\n",
        "plot_2d(zq_umap, chunk_labels, \"UMAP of z_q\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06be3b98",
      "metadata": {
        "id": "06be3b98"
      },
      "outputs": [],
      "source": [
        "index_histograms = []\n",
        "labels = []\n",
        "\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for chunk in chunks:\n",
        "        arr = np.stack(chunk['PWR_ch1'].values)\n",
        "        tensor = torch.tensor(arr, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "        z_e = model.encoder(tensor)\n",
        "        _, _, _, _, indices = model.quantizer(z_e)  # indices: shape (B * H * W)\n",
        "\n",
        "        # Convert to CPU and numpy\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "        # Histogram of indices\n",
        "        hist, _ = np.histogram(indices, bins=np.arange(model.quantizer.num_embeddings + 1))\n",
        "        index_histograms.append(hist)\n",
        "        labels.append(chunk['activity'].iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5pHTfQQVhaLS",
      "metadata": {
        "id": "5pHTfQQVhaLS"
      },
      "outputs": [],
      "source": [
        "X = np.stack(index_histograms)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=50)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=2555304, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X_pca)\n",
        "\n",
        "# UMAP\n",
        "umap_model = umap.UMAP(n_components=2, random_state=2555304)\n",
        "X_umap = umap_model.fit_transform(X_pca)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XQn9jpQ0hixZ",
      "metadata": {
        "id": "XQn9jpQ0hixZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_2d(X_embedded, labels, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=labels, palette='tab10', s=20)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Dim 1\")\n",
        "    plt.ylabel(\"Dim 2\")\n",
        "    plt.legend(title=\"Activity\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"output_images/{title}.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_2d(X_tsne, labels, \"t-SNE of Codebook Index Histograms\")\n",
        "plot_2d(X_umap, labels, \"UMAP of Codebook Index Histograms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76dd4a16",
      "metadata": {
        "id": "76dd4a16"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "980Be_m4dWXS"
      },
      "id": "980Be_m4dWXS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}